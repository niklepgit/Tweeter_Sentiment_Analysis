{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students: Lepidas Nikolas sdi1600090 and Lamprinos Nikos sdi1600088\n",
    "\n",
    "Course: Datamining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The puprose of this exercise is to write a program that will be able to \"know\" if a tweet has positive, negative or neutral sentiment. The steps that we will go through are cleaning, stemming, vectorization and classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will make the cells wider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we create 3 dataframes with every sentiment from the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_file = r'../twitter_data/train2017.tsv'\n",
    "#train_file = r'./small_train.tsv'\n",
    "dataframe = pd.read_csv(train_file, sep='\\t', names=['ID1','ID2','Sentiment','Tweet'])\n",
    "\n",
    "grouped = dataframe.groupby('Sentiment')                        #group by Sentiment\n",
    "positive_tweets = grouped.get_group('positive').reset_index()   #get a dataframe with positive tweets\n",
    "negative_tweets = grouped.get_group('negative').reset_index()   #get a dataframe with negative tweets\n",
    "neutral_tweets = grouped.get_group('neutral').reset_index()     #get a dataframe with neutral tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweets dataframe:\n",
      "\n",
      "   index                 ID1        ID2 Sentiment  \\\n",
      "0      0  264183816548130816   15140428  positive   \n",
      "1      6  264105751826538497  147088367  positive   \n",
      "2     12  263398998675693568  812957996  positive   \n",
      "3     13  263650552167157762  224845471  positive   \n",
      "4     15  264087629237202944   61903760  positive   \n",
      "\n",
      "                                               Tweet  \n",
      "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
      "1  with J Davlar 11th. Main rivals are team Polan...  \n",
      "2  @oluoch @victor_otti @kunjand I just watched i...  \n",
      "3  One of my best 8th graders Kory was excited af...  \n",
      "4  @MsSheLahY I didnt want to just pop up... but ...  \n",
      "\n",
      "\n",
      "Negative tweets dataframe:\n",
      "\n",
      "   index                 ID1        ID2 Sentiment  \\\n",
      "0      1  263405084770172928  591166521  negative   \n",
      "1      2  262163168678248449   35266263  negative   \n",
      "2      3  264249301910310912   18516728  negative   \n",
      "3      7  264094586689953794  332474633  negative   \n",
      "4      9  254941790757601280  557103111  negative   \n",
      "\n",
      "                                               Tweet  \n",
      "0  Theo Walcott is still shit, watch Rafa and Joh...  \n",
      "1  its not that I'm a GSP fan, i just hate Nick D...  \n",
      "2  Iranian general says Israel's Iron Dome can't ...  \n",
      "3  Talking about ACT's && SAT's, deciding where I...  \n",
      "4  They may have a SuperBowl in Dallas, but Dalla...  \n",
      "\n",
      "\n",
      "Neutral tweets dataframe:\n",
      "\n",
      "   index                 ID1        ID2 Sentiment  \\\n",
      "0      4  262682041215234048  254373818   neutral   \n",
      "1      5  264229576773861376  518129399   neutral   \n",
      "2      8  212392538055778304  274996324   neutral   \n",
      "3     10  264169034155696130  382403760   neutral   \n",
      "4     11  263192091700654080  344222239   neutral   \n",
      "\n",
      "                                               Tweet  \n",
      "0  Tehran, Mon Amour: Obama Tried to Establish Ti...  \n",
      "1  I sat through this whole movie just for Harry ...  \n",
      "2  Why is \\\"\"Happy Valentines Day\\\"\" trending? It...  \n",
      "3  Im bringing the monster load of candy tomorrow...  \n",
      "4  Apple software, retail chiefs out in overhaul:...  \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Some printings\n",
    "print(\"Positive tweets dataframe:\\n\")\n",
    "print (positive_tweets.head())\n",
    "print('\\n')\n",
    "print(\"Negative tweets dataframe:\\n\")\n",
    "print (negative_tweets.head())\n",
    "print('\\n')\n",
    "print(\"Neutral tweets dataframe:\\n\")\n",
    "print (neutral_tweets.head())\n",
    "print ('\\n\\n\\n')\n",
    "#end of printings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have a bar chart in order to see how many tweets from our dataset are positive, negative and neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3X1YFXX+//HXkZsDqJwQAsQwsC3zLmvFFK3FVJTU1O1GE0NJM03LEN1a2+601DQzDUs3s+xab3+b0p1FoHmTCaYoJmm1W96uIJp0UFNAnN8fXsy3E2hYHMDm+biuua7mM+/5zGcOp8PLz5wZbIZhGAIAAIBl1KvtAQAAAKBmEQABAAAshgAIAABgMQRAAAAAiyEAAgAAWAwBEAAAwGIIgAAAABZDAAQAALAYAiAAAIDFEAABAAAshgAIAABgMQRAAAAAiyEAAgAAWAwBEAAAwGIIgAAAABZDAAQAALAYAiAAAIDFEAABAAAshgAIAABgMQRAAAAAiyEAAgAAWAwBEAAAwGIIgAAAABZDAAQAALAYAiAAAIDFEAABAAAshgAIAABgMQRAAAAAiyEAAgAAWAwBEAAAwGIIgAAAABZDAAQAALAYAiAAAIDFEAABAAAshgAIAABgMQRAAAAAiyEAApehL7/8UsOHD9c111wjX19f+fr66tprr9XIkSO1bdu22h5ejVi/fr1sNpvWr19/wRqbzVal5WJ9XIozZ87IZrPphRdeqJb+PvzwwyqNv0GDBtVyvOqybds2PfvssyooKKjtoQC4AM/aHgCAS/PPf/5TDz/8sJo3b65HH31UrVq1ks1m0549e7Rs2TK1b99e//3vf3XNNdfU9lBrXWZmpsv6c889p3Xr1unTTz91aW/ZsmW1HM9utyszM1NNmzatlv5uueWWCucQFxen9u3b67nnnjPbPDw8quV41WXbtm2aNGmS7r77bgUHB9f2cABUggAIXEY+//xzjR49Wr1799Y777wjb29vc1vXrl01ZswY/fvf/5avr+9F+/npp5/k5+fn7uHWuo4dO7qsX3nllapXr16F9gspKSmRh4dHlQOWzWarct9VccUVV1Toz9PTU4GBgdV6HADWwyVg4DIydepUeXh46J///KdL+Pu5e+65R2FhYeZ6YmKiGjRooF27dqlHjx5q2LChunXrZm5/88031bZtW/n4+KhRo0b661//qj179rj02aVLF3Xp0qXCsRITExUREWGu79u3TzabTTNmzNCUKVPUtGlT+fj4KCoqSmvXrq2w/3/+8x/Fx8crODhYdrtdLVq00Kuvvlqh7uuvv1ZcXJz8/PwUFBSkUaNG6cSJE7/2cl2StLQ02Ww2rVixQmPHjlXjxo3l4+OjgwcPKi8vT6NGjVKLFi1Uv359hYSEqHv37hVm5yq7BDx//nzZbDZ9/vnnGjFihAIDAxUUFKR77rlHR44cqbbxb9iwQTabTevWrTPbPvnkE9lsNsXExLjUdurUST179nRpW7hwodq1aydfX185HA7169dPX3/9dYXjbNy4UT179pTD4ZCvr686dOigjz/+2Nw+c+ZMPfTQQ5KkNm3amJepP/zwQ0nSpk2bFBsbq6CgIPn4+Cg8PFz9+/dXYWFhtb0WAH4dARC4TJSVlWndunWKiopS48aNL2nfkpIS9e3bV127dtV7772nSZMmSZKmTZum4cOHq1WrVlq1apXmzJmjL7/8UtHR0frPf/7zm8c6d+5cpaWlafbs2Vq8eLHq1aun22+/3SUw7d69W+3bt1dubq5eeuklffjhh+rdu7fGjh1rjk+Sjhw5opiYGOXm5uq1117Tv/71L508eVIPP/zwbx7fxYwfP17Hjh3TG2+8offee08BAQE6duyYvLy8NGnSJH300UdauHChmjRpoltvvbVCCLyQoUOHqmHDhlq2bJmmTJmi9PR03X///dU27ujoaNWvX19r1qwx29asWSNfX19lZmbq1KlTkqSioiJt3bpV3bt3N+uSk5M1cuRIde7cWampqVqwYIH27dunzp0768CBA2bdqlWr1LVrV3l4eOitt97SypUr1bRpU/Xp00cffPCBJGnw4MF67LHHJElLlixRZmamMjMzdcsttyg/P19xcXEqLS3VggUL9Mknn2jGjBlq1KiRzpw5U22vBYAqMABcFvLz8w1Jxr333lth29mzZ43S0lJzOXfunLlt6NChhiTjzTffdNmnsLDQ8PX1NXr16uXSfuDAAcNutxvx8fFmW0xMjBETE1PhuEOHDjWuvvpqc33v3r2GJCMsLMw4ffq02V5UVGQ0atTI6N69u9nWs2dP46qrrjKcTqdLnw8//LDh4+NjHD9+3DAMw3j88ccNm81m5OTkuNTFxsYakox169ZVGNeFDB061Khfv36l2z7++GNDktGjR49f7af89e7cubMxaNAgs/306dOGJGPatGlm27x58wxJRnJysksfkydPNiSZ51lVgYGBxsCBAyvdFhcXZ9x8883m+o033miMGzfO8Pb2NlavXm0YhmG89957hiRj+/bthmEYxpdffmlIMiZNmuTSV0FBgeHv72+MHj3aMAzDKCkpMRo3bmx07drV5f1lGIYRHR1ttGzZssI579q1y6UuLS3NkGRs2rTpks4ZQPVjBhD4A2jXrp28vLzM5aWXXqpQc9ddd7msZ2Zm6vTp00pMTHRpDw8PV9euXSu9ZFtVd955p3x8fMz1hg0b6o477tDGjRtVVlamM2fOaO3atfrrX/8qPz8/nT171lx69eqlM2fOKCsrS5K0bt06tWrVSm3btnU5Rnx8/G8e38X88nWSJMMwlJKSoptuukk+Pj7y9PSUl5eXPv/88wqXyy+kb9++Lus33HCDJJkzbOfOnXN5HcrKyi557N26dVN2drZ+/PFHHT16VDt37lT//v3VsWNHZWRkSDo/KxgUFKQbb7xRkrR69WpJ0pAhQ1yOHxAQoPbt25t3SGdnZysvL09DhgxRWVmZS+3tt9+u3bt36+jRoxcdX+vWreXr66uHH35Yb7755u+aZQbw+xAAgctEUFCQfH19tX///grbli5dqq1bt+r999+vdF8/Pz/5+/u7tP3www+SVOnl5LCwMHP7bxEaGlppW0lJiU6ePKkffvhBZ8+eVUpKiktw9fLyUq9evSRJx44dM8d5of7cobLXY9q0aRo7dqxuvfVWrVq1Slu2bNHWrVvVtWtXnT59ukr9BgYGuqzb7XZJMvd/4oknXF6HVq1aXfLYu3fvbn5VYO3atapfv76io6PVvXt389LwmjVr1LVrV9lsNkkyv4cYGRlZ4Wexdu1a8+dQXpeYmFih7umnn5b0fz+zC2nSpInWr1+viIgIJSUl6brrrlNERIRmzJghwzAu+XwB/HbcBQxcJjw8PNS1a1elp6crLy/PJaiUP8Zk3759le5b/sv+58oDSV5eXoVthw8fVlBQkLnu4+Mjp9NZoe5Cv/Dz8/MrbfP29laDBg3k5eUlDw8PJSQkaMyYMZX2ERkZaY7zQv25Q2Wv1eLFixUXF6dXXnnFpb2y1+S3euSRR3T33Xeb6792J3dl2rZtq6CgIK1Zs0ZnzpxRTEyMvLy81K1bNz399NPavn279uzZo3Hjxpn7lP+c16xZI4fDUaFPLy8vl7rp06era9eulR6//Gd2MTfffLNSU1NVVlamnJwcvfbaa3r88cfVqFEjPfDAA5d8zgB+GwIgcBmZOHGiPv74Y40aNUrvvPOO+cv5t4iOjpavr68WL16se+65x2w/dOiQPv30U5cwEhERoX//+98qLi42Z65++OEHbd68ucLMonT+ZoEXX3zRvAx84sQJffDBB7r11lvl4eEhPz8/3XbbbdqxY4duuOGGC97RLEm33XabZsyYoZ07d7pcBl66dOlvPvdLZbPZzPMut23bNm3fvl3XXXddtRyjSZMmatKkye/qw2azmZfvz5w5Ywa9m2++Wf7+/po4caIkudwA0rt3bz355JPav3+/hg0bdsG+27dvr+DgYOXm5po3eVzIL2c3K+Ph4aF27dpp/vz5WrRokbZv317l8wTw+xEAgctI586d9eqrr+qRRx7Rn//8Zz344INq1aqV6tWrp7y8PK1cuVKSKg1lv3TFFVfoqaee0hNPPKEhQ4Zo0KBB+uGHHzRp0iT5+PjomWeeMWsTEhL0z3/+U/fdd59GjBihH374QTNmzLjgcTw8PBQbG6vk5GSdO3dO06dPV1FRkcvdvXPmzNEtt9yiW2+9VQ899JAiIiJ04sQJ/fe//9UHH3xgPqw5KSlJb775pnr37q3nn39eISEhWrJkSaWPKHGXPn36aObMmXr++efVqVMn7d69W88995zLI3Dqim7duun//b//J0mKjY2VdP7ZgTExMfrggw/UrFkzl5m6G2+8UcnJyRozZox27dqlrl27qmHDhsrLy1NmZqYiIiKUnJwsb29vzZ8/X/fcc49Onjype++9V6GhoTp27Jh27dql/fv3680335R0/vEv0vm7wUeOHClvb29de+21eu+99/Tuu++qT58+ioiIUElJiRYvXqxz586ZYwVQQ2r7LhQAly4nJ8e4//77jcjISMNutxs+Pj7Gn/70J2PIkCHG2rVrXWovduerYRjGG2+8Ydxwww2Gt7e34XA4jH79+hlfffVVhbq3337baNGiheHj42O0bNnSWLFixQXvAp4+fboxadIk46qrrjK8vb2Nm266yfjkk08q9Ll3715j2LBhRpMmTQwvLy/jyiuvNDp16mQ8//zzLnW7d+82YmNjDR8fH6NRo0bG8OHDzbtZq/su4A8++KDCtp9++slISkoyGjdubPj4+BhRUVHG6tWrjYEDBxrNmzc36y52F/Av74gtP15mZmaVx28YF78L2DAM47vvvjPvxP65OXPmGJKMBx98sNL9Fi9ebHTu3Nlo0KCB4ePjYzRr1syIj483Nm/e7FKXmZlp9OvXzwgMDDS8vLyMsLAwIy4uzli8eLFL3aRJk4zw8HCjXr165uuanZ1t3H333UZERITh4+NjBAQEGLfccovx73//+5JeAwC/n80w+OYtgOqxb98+RUZG6sUXX9SECRNqezgAgAvgLmAAAACLIQACAABYDJeAAQAALIYZQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWw18C+R3OnTunw4cPq2HDhpX+/VAAAFD3GIahEydOKCwsTPXqWXMujAD4Oxw+fFjh4eG1PQwAAPAbHDx4UFdddVVtD6NWEAB/h4YNG0o6/waqyt9eBQAAta+oqEjh4eHm73ErIgD+DuWXff39/QmAAABcZqz89S1rXvgGAACwMAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALKbGA+DGjRt1xx13KCwsTDabTe+++665rbS0VI8//rjatGmj+vXrKywsTEOGDNHhw4dd+igsLFRCQoIcDoccDocSEhL0448/utTs2rVLMTEx8vX1VZMmTTR58mQZhuFSs3LlSrVs2VJ2u10tW7ZUamqq+04cAACgjqjxAHjq1Cm1bdtWc+fOrbDtp59+0vbt2/XUU09p+/btWrVqlb799lv17dvXpS4+Pl45OTlKS0tTWlqacnJylJCQYG4vKipSbGyswsLCtHXrVqWkpGjmzJmaNWuWWZOZmamBAwcqISFBO3fuVEJCggYMGKAtW7a47+QBAADqAJvxy2mxmjy4zabU1FT179//gjVbt27VzTffrP3796tp06bas2ePWrZsqaysLHXo0EGSlJWVpejoaH399ddq3ry55s2bp4kTJ+rIkSOy2+2SpBdeeEEpKSk6dOiQbDabBg4cqKKiIn388cfmseLi4hQQEKBly5ZVafxFRUVyOBxyOp3y9/f/Ha8EAACoKfz+ljxrewC/xul0ymaz6YorrpB0fubO4XCY4U+SOnbsKIfDoc2bN6t58+bKzMxUTEyMGf4kqWfPnpo4caL27dunyMhIZWZmaty4cS7H6tmzp2bPnn3BsRQXF6u4uNhcLyoqqq7TBABU4uWMb2t7CKhF42Kvq+0h/GHV6ZtAzpw5o7///e+Kj483E3p+fr6Cg4Mr1AYHBys/P9+sCQkJcdlevv5rNeXbKzNt2jTze4cOh0Ph4eG//eQAAABqSZ0NgKWlpbr33nt17tw5vfbaay7bbDZbhXrDMFzaf1lTfqX712oq67vcxIkT5XQ6zeXgwYNVPyEAAIA6ok5eAi4tLdWAAQO0d+9effrppy7X50NDQ3XkyJEK+xw9etSc0QsNDa0wk1dQUCBJv1rzy1nBn7Pb7S6XlQEAAC5HdW4GsDz8/ec//9GaNWsUGBjosj06OlpOp1NffPGF2bZlyxY5nU516tTJrNm4caNKSkrMmvT0dIWFhSkiIsKsycjIcOk7PT3d7AMAAOCPqsYD4MmTJ5WTk6OcnBxJ0t69e5WTk6MDBw7o7Nmzuvvuu7Vt2zYtWbJEZWVlys/PV35+vhnmWrRoobi4OI0YMUJZWVnKysrSiBEj1KdPHzVv3lzS+cfE2O12JSYmKjc3V6mpqZo6daqSk5PNS7yPPvqo0tPTNX36dH399deaPn261qxZo6SkpJp+SQAAAGpUjT8GZv369brtttsqtA8dOlTPPvusIiMjK91v3bp16tKliyTp+PHjGjt2rN5//31JUt++fTV37lzzTmHp/IOgx4wZoy+++EIBAQEaNWqUnn76aZfv+L3zzjt68skn9f333+uaa67RlClTdOedd1b5XLiNHADci7uArc1ddwHz+7uWnwN4ueMNBADuRQC0NgKg+9S57wACAADAvQiAAAAAFkMABAAAsBgCIAAAgMUQAAEAACyGAAgAAGAxBEAAAACLIQACAABYDAEQAADAYjxrewC4MJ6Ab23uegI+AADMAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxdR4ANy4caPuuOMOhYWFyWaz6d1333XZbhiGnn32WYWFhcnX11ddunTRV1995VJTWFiohIQEORwOORwOJSQk6Mcff3Sp2bVrl2JiYuTr66smTZpo8uTJMgzDpWblypVq2bKl7Ha7WrZsqdTUVPecNAAAQB1S4wHw1KlTatu2rebOnVvp9hkzZmjWrFmaO3eutm7dqtDQUMXGxurEiRNmTXx8vHJycpSWlqa0tDTl5OQoISHB3F5UVKTY2FiFhYVp69atSklJ0cyZMzVr1iyzJjMzUwMHDlRCQoJ27typhIQEDRgwQFu2bHHfyQMAANQBNuOX02I1eXCbTampqerfv7+k87N/YWFhSkpK0uOPPy5JKi4uVkhIiKZPn66RI0dqz549atmypbKystShQwdJUlZWlqKjo/X111+refPmmjdvniZOnKgjR47IbrdLkl544QWlpKTo0KFDstlsGjhwoIqKivTxxx+b44mLi1NAQICWLVtWpfEXFRXJ4XDI6XTK39+/Ol8aSdLLGd9We5+4fIyLva62hwDUOj4Hrc1dn4Pu/v19OahT3wHcu3ev8vPz1aNHD7PNbrcrJiZGmzdvlnR+5s7hcJjhT5I6duwoh8PhUhMTE2OGP0nq2bOnDh8+rH379pk1Pz9OeU15HwAAAH9UdSoA5ufnS5JCQkJc2kNCQsxt+fn5Cg4OrrBvcHCwS01lffz8GBeqKd9emeLiYhUVFbksAAAAl5s6FQDL2Ww2l3XDMFzafrm9KjXlV7p/raayvstNmzbNvPHE4XAoPDy8CmcDAABQt9SpABgaGipJFWbhCgoKzNm60NBQHTlypMK+R48edamprA9Jv1rzy1nBn5s4caKcTqe5HDx48FJODwAAoE6oUwEwMjJSoaGhysjIMNtKSkq0YcMGderUSZIUHR0tp9OpL774wqzZsmWLnE6nS83GjRtVUlJi1qSnpyssLEwRERFmzc+PU15T3kdl7Ha7/P39XRYAAIDLTY0HwJMnTyonJ0c5OTmSzt/4kZOTowMHDshmsykpKUlTp05VamqqcnNzlZiYKD8/P8XHx0uSWrRoobi4OI0YMUJZWVnKysrSiBEj1KdPHzVv3lzS+cfE2O12JSYmKjc3V6mpqZo6daqSk5PNS7yPPvqo0tPTNX36dH399deaPn261qxZo6SkpJp+SQAAAGqUZ00fcNu2bbrtttvM9eTkZEnS0KFDtWjRIj322GM6ffq0Ro8ercLCQnXo0EHp6elq2LChuc+SJUs0duxY8y7evn37ujxX0OFwKCMjQ2PGjFFUVJQCAgKUnJxsHkuSOnXqpOXLl+vJJ5/UU089pWuuuUYrVqxwubsYAADgj6hWnwN4ueM5gHAnngMI8DlodTwH0H3q1HcAAQAA4H4EQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALKbOBcCzZ8/qySefVGRkpHx9fdWsWTNNnjxZ586dM2sMw9Czzz6rsLAw+fr6qkuXLvrqq69c+iksLFRCQoIcDoccDocSEhL0448/utTs2rVLMTEx8vX1VZMmTTR58mQZhlEj5wkAAFBb6lwAnD59uubPn6+5c+dqz549mjFjhl588UWlpKSYNTNmzNCsWbM0d+5cbd26VaGhoYqNjdWJEyfMmvj4eOXk5CgtLU1paWnKyclRQkKCub2oqEixsbEKCwvT1q1blZKSopkzZ2rWrFk1er4AAAA1zbO2B/BLmZmZ6tevn3r37i1JioiI0LJly7Rt2zZJ52f/Zs+erX/84x+68847JUlvv/22QkJCtHTpUo0cOVJ79uxRWlqasrKy1KFDB0nSggULFB0drW+++UbNmzfXkiVLdObMGS1atEh2u12tW7fWt99+q1mzZik5OVk2m612XgAAAAA3q3MzgLfccovWrl2rb7/9VpK0c+dObdq0Sb169ZIk7d27V/n5+erRo4e5j91uV0xMjDZv3izpfIh0OBxm+JOkjh07yuFwuNTExMTIbrebNT179tThw4e1b9++SsdWXFysoqIilwUAAOByU+dmAB9//HE5nU5df/318vDwUFlZmaZMmaJBgwZJkvLz8yVJISEhLvuFhIRo//79Zk1wcHCFvoODg8398/PzFRERUaGP8m2RkZEV9p82bZomTZr0+04QAACgltW5GcAVK1Zo8eLFWrp0qbZv3663335bM2fO1Ntvv+1S98tLtIZhuLRVdgn312rKbwC50OXfiRMnyul0msvBgwcv7eQAAADqgDo3A/i3v/1Nf//733XvvfdKktq0aaP9+/dr2rRpGjp0qEJDQyWdn6Vr3LixuV9BQYE5gxcaGqojR45U6Pvo0aMuNeWzgT/vQ6o4u1jObre7XDIGAAC4HNW5GcCffvpJ9eq5DsvDw8N8DExkZKRCQ0OVkZFhbi8pKdGGDRvUqVMnSVJ0dLScTqe++OILs2bLli1yOp0uNRs3blRJSYlZk56errCwsAqXhgEAAP5I6lwAvOOOOzRlyhStXr1a+/btU2pqqmbNmqW//vWvks5fnk1KStLUqVOVmpqq3NxcJSYmys/PT/Hx8ZKkFi1aKC4uTiNGjFBWVpaysrI0YsQI9enTR82bN5d0/jExdrtdiYmJys3NVWpqqqZOncodwAAA4A+vzl0CTklJ0VNPPaXRo0eroKBAYWFhGjlypJ5++mmz5rHHHtPp06c1evRoFRYWqkOHDkpPT1fDhg3NmiVLlmjs2LHm3cJ9+/bV3Llzze0Oh0MZGRkaM2aMoqKiFBAQoOTkZCUnJ9fcyQIAANQCm8GfvvjNioqK5HA45HQ65e/vX+39v5zxbbX3icvHuNjransIQK3jc9Da3PU56O7f35eDOncJGAAAAO5FAAQAALAYAiAAAIDFEAABAAAshgAIAABgMQRAAAAAiyEAAgAAWAwBEAAAwGIIgAAAABZDAAQAALAYAiAAAIDFEAABAAAshgAIAABgMQRAAAAAiyEAAgAAWAwBEAAAwGIIgAAAABZTpQB44MABGYZRod0wDB04cKDaBwUAAAD3qVIAjIyM1NGjRyu0Hz9+XJGRkdU+KAAAALhPlQKgYRiy2WwV2k+ePCkfH59qHxQAAADcx/NiG5OTkyVJNptNTz31lPz8/MxtZWVl2rJli2688Ub3jhAAAADV6qIBcMeOHZLOzwDu2rVL3t7e5jZvb2+1bdtWEyZMcO8IAQAAUK0uGgDXrVsnSbr//vs1Z84c+fv718igAAAA4D4XDYDl3nrrLXePAwAAADWkSgHw1KlTeuGFF7R27VoVFBTo3LlzLtu///57twwOAAAA1a9KAfCBBx7Qhg0blJCQoMaNG1d6RzAAAAAuD1UKgB9//LFWr16tzp07u3s8AAAAcLMqPQcwICBAjRo1cvdYAAAAUAOqFACfe+45Pf300/rpp5/cPR4AAAC4WZUuAb/00kv67rvvFBISooiICHl5ebls3759u1sGBwAAgOpXpQDYv39/d48DAAAANaRGpDcyAAAgAElEQVRKAfCZZ55x9zgAAABQQ6r0HUAAAAD8cVRpBrBevXoXffZfWVlZtQ0IAAAA7lWlAJiamuqyXlpaqh07dujtt9/WpEmT3DIwAAAAuEeVAmC/fv0qtN19991q1aqVVqxYoeHDh1f7wAAAAOAev+s7gB06dNCaNWuqaywAAACoAb85AJ4+fVopKSm66qqrqnM8AAAAcLMqXQIOCAhwuQnEMAydOHFCfn5+Wrx4sdsGBwAAgOpXpQA4e/Zsl/V69erpyiuvVIcOHRQQEOCWgQEAAMA9qhQAhw4d6u5xAAAAoIZUKQBK0o8//qiFCxdqz549stlsatmypYYNGyaHw+HO8QEAAKCaVekmkG3btumaa67Ryy+/rOPHj+vYsWOaNWuWrrnmGm3fvt3dYwQAAEA1qtIM4Lhx49S3b18tWLBAnp7ndzl79qweeOABJSUlaePGjW4dJAAAAKpPlQLgtm3bXMKfJHl6euqxxx5TVFSU2wYHAACA6lelS8D+/v46cOBAhfaDBw+qYcOG1T4oAAAAuE+VAuDAgQM1fPhwrVixQgcPHtShQ4e0fPlyPfDAAxo0aJC7xwgAAIBqVKUAOHPmTN15550aMmSIIiIidPXVVysxMVF33323pk+fXu2D+t///qf77rtPgYGB8vPz04033qjs7Gxzu2EYevbZZxUWFiZfX1916dJFX331lUsfhYWFSkhIkMPhkMPhUEJCgn788UeXml27dikmJka+vr5q0qSJJk+eLMMwqv18AAAA6pIqBUBvb2/NmTNHhYWFysnJ0Y4dO3T8+HG9/PLLstvt1TqgwsJCde7cWV5eXvr444+1e/duvfTSS7riiivMmhkzZmjWrFmaO3eutm7dqtDQUMXGxurEiRNmTXx8vHJycpSWlqa0tDTl5OQoISHB3F5UVKTY2FiFhYVp69atSklJ0cyZMzVr1qxqPR8AAIC6psrPAZQkPz8/tWnTxl1jkSRNnz5d4eHheuutt8y2iIgI878Nw9Ds2bP1j3/8Q3feeack6e2331ZISIiWLl2qkSNHas+ePUpLS1NWVpY6dOggSVqwYIGio6P1zTffqHnz5lqyZInOnDmjRYsWyW63q3Xr1vr22281a9YsJScnu/zpOwAAgD+SKs0AnjlzRi+++KJ69eqlqKgo/fnPf3ZZqtP777+vqKgo3XPPPQoODtZNN92kBQsWmNv37t2r/Px89ejRw2yz2+2KiYnR5s2bJUmZmZlyOBxm+JOkjh07yuFwuNTExMS4zGD27NlThw8f1r59+6r1nAAAAOqSKs0ADhs2TBkZGbr77rt18803u3V27Pvvv9e8efOUnJysJ554Ql988YXGjh0ru92uIUOGKD8/X5IUEhLisl9ISIj2798vScrPz1dwcHCFvoODg8398/PzXWYWf95nfn6+IiMjK+xfXFys4uJic72oqOi3nygAAEAtqVIAXL16tT766CN17tzZ3ePRuXPnFBUVpalTp0qSbrrpJn311VeaN2+ehgwZYtb9MoQahuHSVllI/bWa8htALhRwp02bpkmTJl3iGQEAANQtVboE3KRJkxp73l/jxo3VsmVLl7YWLVqYzyEMDQ2VJHMmr1xBQYE5gxcaGqojR45U6Pvo0aMuNZX1IVWcXSw3ceJEOZ1Oczl48OClnh4AAECtq1IAfOmll/T444+bl1jdqXPnzvrmm29c2r799ltdffXVkqTIyEiFhoYqIyPD3F5SUqINGzaoU6dOkqTo6Gg5nU598cUXZs2WLVvkdDpdajZu3KiSkhKzJj09XWFhYRUuDZez2+3y9/d3WQAAAC43VQqAUVFROnPmjJo1a6aGDRuqUaNGLkt1GjdunLKysjR16lT997//1dKlS/X6669rzJgxks5fnk1KStLUqVOVmpqq3NxcJSYmys/PT/Hx8ZLOzxjGxcVpxIgRysrKUlZWlkaMGKE+ffqoefPmks4/JsZutysxMVG5ublKTU3V1KlTuQMYAAD84VXpO4CDBg3S//73P02dOlUhISFuDUjt27dXamqqJk6cqMmTJysyMlKzZ8/W4MGDzZrHHntMp0+f1ujRo1VYWKgOHTooPT3d5TL1kiVLNHbsWPNu4b59+2ru3LnmdofDoYyMDI0ZM0ZRUVEKCAhQcnKykpOT3XZuAAAAdYHNqMKfvvDz81NmZqbatm1bE2O6bBQVFcnhcMjpdLrlcvDLGd9We5+4fIyLva62hwDUOj4Hrc1dn4Pu/v19OajSJeDrr79ep0+fdvdYAAAAUAOqFABfeOEFjR8/XuvXr9cPP/ygoqIilwUAAACXjyp9BzAuLk6S1K1bN5f28ufqlZWVVf/IAAAA4BZVCoDr1q274LYdO3ZU22AAAADgflUKgDExMS7rTqdTS5Ys0RtvvKGdO3cqKSnJLYMDAABA9avSdwDLffrpp7rvvvvUuHFjpaSkqFevXtq2bZu7xgYAAAA3+NUZwEOHDmnRokV68803derUKQ0YMEClpaVauXJlhT/ZBgAAgLrvojOAvXr1UsuWLbV7926lpKTo8OHDSklJqamxAQAAwA0uOgOYnp6usWPH6qGHHtK1115bU2MCAACAG110BvCzzz7TiRMnFBUVpQ4dOmju3Lk6evRoTY0NAAAAbnDRABgdHa0FCxYoLy9PI0eO1PLly9WkSROdO3dOGRkZOnHiRE2NEwAAANWkSncB+/n5adiwYdq0aZN27dql8ePH64UXXlBwcLD69u3r7jECAACgGl3SY2AkqXnz5poxY4YOHTqkZcuWuWNMAAAAcKNLDoDlPDw81L9/f73//vvVOR4AAAC42W8OgAAAALg8EQABAAAshgAIAABgMQRAAAAAiyEAAgAAWAwBEAAAwGIIgAAAABZDAAQAALAYAiAAAIDFEAABAAAshgAIAABgMQRAAAAAiyEAAgAAWAwBEAAAwGIIgAAAABZDAAQAALAYAiAAAIDFEAABAAAshgAIAABgMQRAAAAAiyEAAgAAWAwBEAAAwGIIgAAAABZDAAQAALAYAiAAAIDFeNb2AADUXS9nfFvbQ0AtGhd7XW0PAYCbMAMIAABgMQRAAAAAiyEAAgAAWAwBEAAAwGIIgAAAABZDAAQAALAYAiAAAIDFEAABAAAsps4HwGnTpslmsykpKclsKy4u1iOPPKKgoCDVr19fffv21aFDh1z2O3DggO644w7Vr19fQUFBGjt2rEpKSlxqNmzYoHbt2snHx0fNmjXT/Pnza+ScAAAAalOdDoBbt27V66+/rhtuuMGlPSkpSampqVq+fLk2bdqkkydPqk+fPiorK5MklZWVqXfv3jp16pQ2bdqk5cuXa+XKlRo/frzZx969e9WrVy/deuut2rFjh5544gmNHTtWK1eurNFzBAAAqGl19k/BnTx5UoMHD9aCBQv0/PPPm+1Op1MLFy7Uv/71L3Xv3l2StHjxYoWHh2vNmjXq2bOn0tPTtXv3bh08eFBhYWGSpJdeekmJiYmaMmWK/P39NX/+fDVt2lSzZ8+WJLVo0ULbtm3TzJkzddddd9X8CQMAANSQOjsDOGbMGPXu3dsMeeWys7NVWlqqHj16mG1hYWFq3bq1Nm/eLEnKzMxU69atzfAnST179lRxcbGys7PNmp/3UV6zbds2lZaWVjqm4uJiFRUVuSwAAACXmzoZAJcvX67t27dr2rRpFbbl5+fL29tbAQEBLu0hISHKz883a0JCQly2BwQEyNvb+6I1ISEhOnv2rI4dO1bpuKZNmyaHw2Eu4eHhv/kcAQAAakudC4AHDx7Uo48+qsWLF8vHx6fK+xmGIZvNZq7//L+rWmMYxgX3laSJEyfK6XSay8GDB6s8PgAAgLqizgXA7OxsFRQUqF27dvL09JSnp6c2bNigV155RZ6engoJCVFJSYkKCwtd9isoKDBn9EJDQ82ZvnKFhYUqLS29aE1BQYE8PT0VGBhY6djsdrv8/f1dFgAAgMtNnQuA3bp1065du5STk2MuUVFRGjx4sPnfXl5eysjIMPfJy8tTbm6uOnXqJEmKjo5Wbm6u8vLyzJr09HTZ7Xa1a9fOrPl5H+U15f0DAAD8UdW5u4AbNmyo1q1bu7TVr19fgYGBZvvw4cM1fvx4BQYGqlGjRpowYYLatGlj3jDSo0cPtWzZUgkJCXrxxRd1/PhxTZgwQSNGjDBn7UaNGqW5c+cqOTlZI0aMUGZmphYuXKhly5bV7AkDAADUsDoXAKvi5ZdflqenpwYMGKDTp0+rW7duWrRokTw8PCRJHh4eWr16tUaPHq3OnTvL19dX8fHxmjlzptlHZGSkPvroI40bN06vvvqqwsLC9Morr/AIGAAA8Id3WQTA9evXu6z7+PgoJSVFKSkpF9ynadOm+vDDDy/ab0xMjLZv314dQwQAALhs1LnvAAIAAMC9CIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIupcwFw2rRpat++vRo2bKjg4GD1799f33zzjUtNcXGxHnnkEQUFBal+/frq27evDh065FJz4MAB3XHHHapfv76CgoI0duxYlZSUuNRs2LBB7dq1k4+Pj5o1a6b58+e7/fwAAABqW50LgBs2bNCYMWOUlZWljIwMnT17Vj169NCpU6fMmqSkJKWmpmr58uXatGmTTp48qT59+qisrEySVFZWpt69e+vUqVPatGmTli9frpUrV2r8+PFmH3v37lWvXr106623aseOHXriiSc0duxYrVy5ssbPGQAAoCZ51vYAfiktLc1l/a233lJwcLCys7P1l7/8RU6nUwsXLtS//vUvde/eXZK0ePFihYeHa82aNerZs6fS09O1e/duHTx4UGFhYZKkl156SYmJiZoyZYr8/f01f/58NW3aVLNnz5YktWjRQtu2bdPMmTN111131exJAwAA1KA6NwP4S06nU5LUqFEjSVJ2drZKS0vVo0cPsyYsLEytW7fW5s2bJUmZmZlq3bq1Gf4kqWfPniouLlZ2drZZ8/M+ymu2bdum0tJSt54TAABAbapzM4A/ZxiGkpOTdcstt6h169aSpPz8fHl7eysgIMClNiQkRPn5+WZNSEiIy/aAgAB5e3tftCYkJERnz57VsWPH1Lhx4wrjKS4uVnFxsbleVFT0+08SAACghtXpGcCHH35YX375pZYtW/artYZhyGazmes//++q1hiGccF9pfM3qDgcDnMJDw+v0nkAAADUJXU2AD7yyCN6//33tW7dOl111VVme2hoqEpKSlRYWOhSX1BQYM7ohYaGmjN95QoLC1VaWnrRmoKCAnl6eiowMLDSMU2cOFFOp9NcDh48+LvPEwAAoKbVuQBoGIYefvhhrVq1Sp9++qkiIyNdtrdr105eXl7KyMgw2/Ly8pSbm6tOnTpJkqKjo5Wbm6u8vDyzJj09XXa7Xe3atTNrft5HeU1UVJS8vLwqHZvdbpe/v7/LAgAAcLmpcwFwzJgxWrx4sZYuXaqGDRsqPz9f+fn5On36tCTJ4XBo+PDhGj9+vNauXasdO3bovvvuU5s2bcy7gnv06KGWLVsqISFBO3bs0Nq1azVhwgSNGDHCDG2jRo3S/v37lZycrD179ujNN9/UwoULNWHChFo7dwAAgJpQ5wLgvHnz5HQ61aVLFzVu3NhcVqxYYda8/PLL6t+/vwYMGKDOnTvLz89PH3zwgTw8PCRJHh4eWr16tXx8fNS5c2cNGDBA/fv318yZM80+IiMj9dFHH2n9+vW68cYb9dxzz+mVV17hETAAAOAPr87dBVx+I8bF+Pj4KCUlRSkpKResadq0qT788MOL9hMTE6Pt27df8hgBAAAuZ3VuBhAAAADuRQAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiCIAAAAAWQwAEAACwGAIgAACAxRAAAQAALIYACAAAYDEEQAAAAIshAAIAAFgMARAAAMBiLB8AX3vtNUVGRsrHx0ft2rXTZ599VttDAgAAcCtLB8AVK1YoKSlJ//jHP7Rjxw7deuutuv3223XgwIHaHhoAAIDbWDoAzpo1S8OHD9cDDzygFi1aaPbs2QoPD9e8efNqe2gAAABu41nbA6gtJSUlys7O1t///neX9h49emjz5s2V7lNcXKzi4mJz3el0SpKKiorcMsYzp066pV9cHtz1vroUvAetjfcgapu73oPl/RqG4Zb+LweWDYDHjh1TWVmZQkJCXNpDQkKUn59f6T7Tpk3TpEmTKrSHh4e7ZYywtidqewCwPN6DqG3ufg+eOHFCDofDzUepmywbAMvZbDaXdcMwKrSVmzhxopKTk831c+fO6fjx4woMDLzgPvhtioqKFB4eroMHD8rf37+2hwML4j2I2sZ70H0Mw9CJEycUFhZW20OpNZYNgEFBQfLw8Kgw21dQUFBhVrCc3W6X3W53abviiivcNkZI/v7+fPChVvEeRG3jPegeVp35K2fZm0C8vb3Vrl07ZWRkuLRnZGSoU6dOtTQqAAAA97PsDKAkJScnKyEhQVFRUYqOjtbrr7+uAwcOaNSoUbU9NAAAALexdAAcOHCgfvjhB02ePFl5eXlq3bq1PvroI1199dW1PTTLs9vteuaZZypccgdqCu9B1Dbeg3Anm2Hle6ABAAAsyLLfAQQAALAqAiAAAIDFEAABAAAshgCIKklMTFT//v0vWrN+/XrZbDb9+OOPNTQq4NdFRERo9uzZtT0M4FfxGYqaRAD8g0hMTJTNZpPNZpOXl5eaNWumCRMm6NSpU9XS/5w5c7Ro0SJzvUuXLkpKSnKp6dSpk/Ly8tz2cM1nn33WPMcLLfv27XPLsS8mLS1NNptNZ86cqfFj16by99wLL7zg0v7uu+/Wyl/GWbRoUaUPZt+6dasefPBBtxxz3759v/qefPbZZ91y7F8TGhqq+fPn18qxa1NNvi/Lf/45OTnV2m9lysPhxZaff0bXlDNnzshmsyktLa3Gj43fx9KPgfmjiYuL01tvvaXS0lJ99tlneuCBB3Tq1CnNmzfvd/ddlVDn7e2t0NDQ332sC5kwYYLLMxrbt2+vBx98UCNGjDDbrrzySrcdHxX5+Pho+vTpGjlypAICAmp7OJVy53siPDxceXl55vrMmTOVlpamNWvWmG0NGjRw2/FRubr2viwpKZG3t/fv6qP8H9jlHn30URUVFemtt94y26z+ly1waZgB/AOx2+0KDQ1VeHi44uPjNXjwYL377rvm9g0bNujmm2+W3W5X48aN9fe//11nz541t7/zzjtq06aNfH19FRgYqO7du5sziD+/BJyYmKgNGzZozpw5LjNvP7984XQ65evrW+FfhatWrVL9+vV18uRJSdL//vc/DRw4UAEBAQoMDFS/fv0uOIvXoEEDhYaGmouHh4caNmzo0rZ9+3Z5enqqqKhIkpSfny+bzaaEhASzn2eeeUa33Xabub5r1y717NlT9evXV+PGjTVs2DAVFhaa28+dO6cpU6YoIiJCfn5+uummm/Tee+9Jkr7++mvdfvvtkiRfX1/ZbDYzpC5btkytWrWSj4+PgoKC1KNHDxUXF1/CT7Tu6969u0JDQzVt2rSL1m3evFl/+ctf5Ovrq/DwcI0dO9ZldjovL0+9e/eWr6+vIiMjtXTp0gqXbmfNmqU2bdqofv36Cg8P1+jRo8330fr163X//ffL6XRWmHn7eT+DBg3Svffe6zK20tJSBQUFmb9IDcPQjBkz1KxZM/n6+qpt27Z65513Kj0vDw8Pl/dfgwYN5OnpWaGtVatWevXVV8394uLiZLfbdfr0aUn/N5O0f/9+SednVZKTkxUWFqYGDRqoU6dO+vzzz12OvXHjRnXu3Fm+vr5q2rSpxo8fb/bXsWNHHTlyRA899JBsNpt8fHwkSd9995169eqlK664QvXr19cNN9zgElb/KKrrfWmz2Vw+Q6Xzf/6zfKYtMjJSknTTTTfJZrOpS5cukv7v83LatGkKCwvTddddJ0lavHixoqKizM+t+Ph4FRQUVOmcyv+BXb74+vqan/nli7e3txwOh1avXm3ud/311ys8PNxcX7dunXx8fMzPouPHj2vYsGEKCgqSw+FQbGysvvrqK5djr1q1SjfeeKN8fHz0pz/9SVOnTlVZWZmk8/9/SdLtt98um82m66+/XpKUnZ2tv/zlL2rQoIH8/f3Vvn177dy5s0rnippBAPwD8/X1VWlpqaTzQatXr17m/4Tz5s3TwoUL9fzzz0s6/wt40KBBGjZsmPbs2aP169frzjvvVGWPiZwzZ46io6M1YsQI5eXlKS8vz+UDRjr/L9HevXtryZIlLu1Lly5Vv3791KBBA/3000+67bbb1KBBA23cuFGbNm1SgwYNFBcXp5KSkt90zn/+859Vv359ffbZZ5LOh96goCBt2LDBrFm/fr1iYmIkSQcPHlRMTIyio6O1fft2ffjhh/r+++81ePBgs/5vf/ubli9frjfeeEO5ubkaPXq0BgwYoKysLF177bVaunSppPO/xPPy8jRjxgzt379fCQkJGj16tL755ht9+umnuuOOO37TOdVlHh4emjp1qlJSUnTo0KFKa8oD9p133qkvv/xSK1as0KZNm/Twww+bNUOGDNHhw4e1fv16rVy5Uq+//nqFX4z16tXTK6+8otzcXL399tv69NNP9dhjj0k6Pzsye/Zs+fv7m+/JCRMmVBjL4MGD9f7775vBUZI++eQTnTp1SnfddZck6cknn9Rbb72lefPm6auvvtK4ceN03333ubyHLlWXLl20fv16SdLZs2f1+eefy9/fX5mZmZLO/1K++uqrzYfQDx48WNnZ2XrnnXe0c+dO9enTR7GxseY/jrKzs9WrVy8NGjRIu3bt0pIlS5SRkaHk5GRJ0kcffaQrr7xS06dPV15enhksR44cqXr16mnTpk368ssv9fzzz8vX1/c3n1ddVV3vy1/zxRdfSJLWrFmjvLw8rVq1yty2du1a7dmzRxkZGfrwww8lnZ8JfO6557Rz5069++672rt3rxITE3/7if6Ch4eHbrnlFvO9duTIEe3du1dOp1Pff/+9pPOffx07dpTdbldZWZni4uLkdDqVnp6urVu36vrrr1e3bt3Mf0S///77Gj58uCZMmKDdu3dr7ty5mj9/vmbOnCnp/FcspPOf7Xl5edq0aZOk839o4dprr1V2dra2bdumCRMmyNOTi451ioE/hKFDhxr9+vUz17ds2WIEBgYaAwYMMAzDMJ544gmjefPmxrlz58yaV1991WjQoIFRVlZmZGdnG5KMffv2Van/mJgY49FHH3WpWbdunSHJKCwsNAzDMFatWmU0aNDAOHXqlGEYhuF0Og0fHx9j9erVhmEYxsKFCyuMqbi42PD19TU++eSTXz3nq6++2nj55ZcrtPfq1cuYMGGCYRiGMWrUKGPixImGv7+/8d133xmnT5/+/+3df1SO9/8H8Oed7vuuk3RGxSjltzhNyzoodM+POjiHsylDycZZrCHNzPGrZhaZzQ4xdnAsS37NTCeMMt1SVlLY6UYk5ddGNFZK5X5+/vDpOt3uGl98vjb36/Ff9/t9Xfd1ve+X635d1/v1vlGr1fLw4cMkydmzZ3PkyJEm258/f54AWFJSwvLycqrVaubl5Zn0CQkJ4XvvvUeS3L9/PwGwqqpKac/MzKRKpeL169cfex7/Vg1jom/fvpw0aRJJcvfu3Wx4aZkwYQLDw8NNts3IyKCVlRWrqqp45swZAuDx48eV9vrPoLHPt96OHTvYqlUr5e9NmzbRwcHBrF/DOKmpqaGjoyM3b96stI8bN47BwcEkyYqKCtrY2DArK8tkH5MnT+a4ceP+fkBIxsTEsFevXo0eq5OTE41GI3/99Ve6uLgwIiKCCxcuJEmGhYVx4sSJJMmCggI2a9aMN2/eNNmHn58fFy1aRJIMDg7mjBkzTNpTU1OpVqtZW1tLkmzdujXXrl1r0qdLly6Mi4t77Hn8mz2vuCRJANy9e7dJHwcHB27atIkkWVxcTADMz883O4bWrVvz/v37f3usOTk5BMC//vqLpPk19EnPs6EvvviCb7zxBkly27Zt7N+/P4cPH86NGzeSJAcOHMiYmBiS5N69e9mqVSvW1NQo2xuNRrq4uDAhIYEk6ePjwxUrVpi8x/r169mhQweSZFVVFQFw//79JvvQarXctm3bY89DvDiSjr9EUlJS0Lx5c9TV1aG2thajRo1CfHw8AODMmTPo16+fSRG0n58fKioqcOXKFfTq1QuDBw+Gp6cnAgMDERAQgKCgoGeqnxkxYgSsra2RnJyMsWPHYteuXbC3t0dAQACAhzX7F2gAAAxDSURBVE8xLly4AHt7e5PtqqurUVRU9NTvq9PpsGPHDgAPnwDGx8fj1KlT0Ov1ynRF3759lWPIyMhotE6rqKgI1tbWqK2txYABA0zaampq0K9fvyaPwcfHB/3790f37t1NxvNlrdFZtmwZBg0ahFmzZpm11X/ODZ8Gk4TRaERxcTEKCwthbW0Nb29vpb1z585msXf48GEsWbIEBoMBd+/eRV1dHaqrq1FZWQk7O7snOk61Wo3g4GBs2bIFEyZMQGVlJfbs2aM8xTUYDKiursbQoUNNtqupqcHrr7/+xOPxKJ1Oh7KyMhgMBuj1euh0Ouh0OqxevRrAwziNiYkB8HC8jEajEqv17t+/j06dOil9rl69io0bNyrtJFFbW4vLly8rU5OPmjlzJiIjI5GSkoIhQ4YgKCgIPXv2fOrz+qd7lrj08PB4pvf29PQ0q/vLz8/Hp59+ipMnT+L27dswGo0AgNLSUvTo0eOZ3q+eTqfD3LlzcffuXSXWWrRoAb1ej/HjxyM7O1spjzhx4gTKy8vN/q1VVVWhqKgIJJGfn4/ffvsNCxcuVNofPHiAmpoakxKihlQqFWbOnInQ0FBs3LgRQ4YMwZgxY8xiWrxYkgC+RN58802sXbsWarUabdu2hVqtVtpImq2A43+nd1UqFZo1a4bU1FRkZWXh4MGDiI+Px/z585Gdnd3kl8njaDQaBAUFISkpCWPHjkVSUhLeeecdZRrAaDSid+/eZtPEwLMV7tdfAC9cuICLFy/C19cXJ06cgF6vx6VLl9CnTx+lJspoNCIoKAifffaZ2X7atm2rTG+kpaXB0dHRpL1+H41Rq9VIT09HZmYmDh48iK+//hoLFizA8ePH4eLi8tTn9k81cOBABAYGYt68eWZTWkajEVOmTMGMGTPMtmvfvj3OnTvX6D7ZoPygpKQEw4cPx9SpU7F48WK0bNkSR48exeTJk5UyhycVEhICf39/3LhxA6mpqbCxsVHqOOu/kPfu3Yt27dqZbPcs/x+rk5MTPDw8kJ6eDr1ej7fffhv+/v4ICwvD2bNnUVJSotSPGY1GaDSaRleW1t8sGY1GTJ8+HVOmTDHr83fxFRERgREjRmDv3r04cOAAYmNjsXr16v/ZKukX7VniEnh4beQjZTBPGm+P3pRUVlYiICAAAQEBSExMhJOTE0pLSxEYGPjUJS+NaVgGo9frsWrVKtjb22PNmjU4duwYSCo3r/U3GgcOHDDbzyuvvKIkxHFxcRgxYoRZn2bNmjWZBMbFxWHixInYt28f9u3bh+joaOzatavR/YgXQxLAl4idnR06d+7caFuPHj2wa9cuk0QwKysL9vb2yhedSqWCn58f/Pz8EB0dDTc3N+zevVupK2pIo9EoRcB/JyQkBAEBASgoKMDhw4exePFipc3b2xvbt2+Hs7MzWrRo8TSn3Kj6C+Dnn38OHx8f2Nrawt/fH9988w3c3NyU+r/6vqmpqejYsSOsrMxLYj09PWFtbY3Lly8rTw0fVX+X/+h4WFlZYcCAARgwYACio6PRrl07JCcnIyIi4rmd6z9JXFwcvLy8lIL3et7e3igoKGgyNrt37466ujrk5+ejd+/eAIALFy6Y/BZabm4u6urq8NVXXymfU/1T3npPGpO+vr5wdXXF9u3bsX//fgQHByufYY8ePaDValFaWmoSJ8+DTqfDoUOHcPToUaxatQrOzs5wd3fH0qVL4erqqtxoeXt74/79+ygvL4ePj0+j+/L29obBYGhyTIGmx8PNzQ0RERGIiIhAVFQUNmzY8NImgMDTxyXwMHFvuPL2/PnzuHfvnvJ3U//2G3P27FmUlZUhLi5OqZnOzc39P53Lk6ivA9y5cyeKiorg6+sLtVqN27dv47vvvjO5Afb29saSJUtga2trdsNTz8vLC4WFhYiMjGy03draGiqVqtEx8PDwgIeHB2bNmoW33noLCQkJkgD+g8giEAsRERGBy5cvY/r06Th79iz27NmDmJgYfPTRR7CyskJ2djaWLFmC3NxclJaW4scff8TNmzebnAZxd3dHdnY2Ll26hLKyMuXJyaP8/f3RunVrhISEwN3d3SSJCgkJgaOjI0aNGoWMjAwUFxdDr9cjMjKyycLtJ1F/AUxMTFSeqvTu3Ru3bt1CZmam8hoA5b1CQ0ORm5uLoqIi/Pzzz5g0aRIAoGXLloiMjMS0adOQmJiIoqIi5OXlYeXKlcq0Yf20RkpKCm7evInKykpkZGRg2bJlOHHiBEpKSrBz506Ul5c/87TSP5mnpydCQkKUsoN6c+bMwbFjx/Dhhx/i5MmTOH/+PJKTkzF9+nQADxPAIUOGIDw8HDk5OcjPz0d4eLiyqhoAOnXqhLq6OsTHx+PixYv4/vvvzX7jzt3dHRUVFTh06BDKyspMvqgbUqlUGD9+PNatW4fU1FSEhoYqbfb29vj4448RFRWFhIQEFBUVIT8/H2vWrEFCQsIzjY9Op0NycjJatGihTOXqdDps2bLFJCY9PT0xevRojBs3Dnv27EFxcTFycnIQGxuL1NRUAMC8efOQlpaGqKgonDp1CoWFhfjpp58QFRVlMh7p6em4du0abt26BQCYNm0aUlNTUVxcjNzcXOj1+pc6JoGnj0sAGDRoEFavXo28vDzk5uZi6tSpJjMrzs7Oyq8d/PHHH7hz506Tx9G+fXtoNBolhpOTk01uiJ8nnU6HxMRE5QbY2toafn5+ZrE2fPhweHl5YdSoUUhLS0NxcTEyMzMxd+5cnD59GsDDX01Yv349YmNjYTAYYDAYsHXrVixatAjAwwTQxcUFaWlp+P3335VfgYiMjMSRI0dQUlKCjIwM5OXlvfSx9q/zgmoPxXPWVEFwQ+np6fTx8aFGo2GbNm04Z84cpWDcYDAwMDCQTk5O1Gq17Nq1K+Pj45vc/7lz59i3b1/a2toSAIuLi5ssYJ49ezYBMDo62uyYrl+/zrCwMDo6OlKr1bJjx458//33eefOnceec1OLQEhy+fLlBMC0tDTltcDAQGo0Gt67d8+kr8Fg4MiRI+ng4EBbW1t6eHgoi0hI8sGDB/zyyy/ZtWtXqtVqOjs7c9iwYczMzFT6zJ8/n87OzlSpVJwyZQpPnz7NoUOH0tHRkTY2NuzevTu//fbbx57Tv0ljMXfp0iVqtVo+emnJycnh0KFD2bx5c9rZ2fG1115jbGys0n7t2jUOGzaMWq2Wbm5uTEpKorOzM9etW6f0WbFiBV999VXa2toyMDCQmzdvNou3qVOnslWrVgSgFLo3FicFBQUEQDc3N5NFSOTDAvaVK1eyW7duVKvVdHJyYmBgIPV6/WPHpKlFICR548YNqlQqhoaGKq9t3bqVALhhwwaTvtXV1Zw7dy7d3NyoVqvZtm1bjh49mgaDQemTlZXFQYMG0c7Ojs2bN6eXlxeXL1+utB85coQ9e/akRqOhVqslSYaHh7NDhw7UarV0dnbmu++++0QLDv5NnmdcXr16lQEBAbSzs2OXLl24b98+k0Ug5MMFEa6urrSysqK/v3+Tx0CSSUlJdHd3p1arZb9+/ZicnGyyiOR5LAIhyePHjxMAFyxYoLy2dOlSs2siSf7555/84IMP2KZNG2o0GrZv355hYWG8du2a0iclJYV9+vShjY0NHRwc2KdPH5Mx+OGHH9ixY0daW1uzW7durKys5JgxY+ji4kKNRsN27dpx5syZj10UI/5/qchGfudDCCFeoCtXrsDV1RVpaWkYPHjwiz4cIYR46UgCKIR44X755RdUVFTA09MT169fxyeffIKrV6+isLDQZMpNCCHE8yGLQIQQL1xtbS3mzZuHixcvwt7eHr6+vtiyZYskf0II8T8iTwCFEEIIISyMrAIWQgghhLAwkgAKIYQQQlgYSQCFEEIIISyMJIBCCCGEEBZGEkAhhBBCCAsjCaAQQgghhIWRBFAIIYQQwsJIAiiEEEIIYWEkARRCCCGEsDCSAAohhBBCWBhJAIUQQgghLIwkgEIIIYQQFkYSQCGEEEIICyMJoBBCCCGEhZEEUAghhBDCwkgCKIQQQghhYSQBFEIIIYSwMJIACiGEEEJYGEkAhRBCCCEsjCSAQgghhBAWRhJAIYQQQggLIwmgEEIIIYSFkQRQCCGEEMLCSAIohBBCCGFhJAEUQgghhLAwkgAKIYQQQlgYSQCFEEIIISyMJIBCCCGEEBbmP3dn7ohcRfx4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./images/bar_charts/amount_grouped_tweets.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make 3 lists each for every sentiment. So we have positive_tweet list, negative_tweet list and neutral_tweet list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet = []                                                        #create empty positive_tweet list\n",
    "for x in range(len(positive_tweets)):                                      #for every positive tweet in the previous positives_tweet dataframe\n",
    "    positive_tweet.append(positive_tweets['Tweet'][x])                     #append only the Tweet column to the list\n",
    "\n",
    "negative_tweet = []\n",
    "for x in range(len(negative_tweets)):\n",
    "    negative_tweet.append(negative_tweets['Tweet'][x])\n",
    "    \n",
    "neutral_tweet = []\n",
    "for x in range(len(neutral_tweets)):\n",
    "    neutral_tweet.append(neutral_tweets['Tweet'][x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will clean the tweets from urls, tags, symbols and numbers. We also use lower() function to have only lower letters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                                                                                           #import regular expressions\n",
    "\n",
    "for x in range(len(positive_tweet)):                                                                                #for every tweet in the list of positive tweets\n",
    "    positive_tweet[x] = re.sub('http://\\S+|https://\\S+', '', positive_tweet[x])                                     #remove the urls\n",
    "    positive_tweet[x] = re.sub('@\\S+', '', positive_tweet[x])                                                       #remove the tags\n",
    "    positive_tweet[x] = re.sub('[0-9`~!@#\\$%^&\\*\\(\\)\\-\\_\\=\\+\\[\\]\\{\\}\\\\\\|\\;\\:<\\,.\\>\\/\\?\\'\\\"]','', positive_tweet[x]) #remove the symbols\n",
    "    positive_tweet[x] = positive_tweet[x].lower()                                                                   #lowercase the letters\n",
    "\n",
    "for x in range(len(negative_tweet)):\n",
    "    negative_tweet[x] = re.sub('http://\\S+|https://\\S+', '', negative_tweet[x])\n",
    "    negative_tweet[x] = re.sub('@\\S+', '', negative_tweet[x])                                                       \n",
    "    negative_tweet[x] = re.sub('[0-9`~!@#\\$%^&\\*\\(\\)\\-\\_\\=\\+\\[\\]\\{\\}\\\\\\|\\;\\:<\\,.\\>\\/\\?\\'\\\"]','', negative_tweet[x])\n",
    "    negative_tweet[x] = negative_tweet[x].lower()\n",
    "\n",
    "for x in range(len(neutral_tweet)):\n",
    "    neutral_tweet[x] = re.sub('http://\\S+|https://\\S+', '', neutral_tweet[x])\n",
    "    neutral_tweet[x] = re.sub('@\\S+', '', neutral_tweet[x])                                                       \n",
    "    neutral_tweet[x] = re.sub('[0-9`~!@#\\$%^&\\*\\(\\)\\-\\_\\=\\+\\[\\]\\{\\}\\\\\\|\\;\\:<\\,.\\>\\/\\?\\'\\\"]','', neutral_tweet[x])\n",
    "    neutral_tweet[x] = neutral_tweet[x].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use function printList to print only 5 elements from the given list. We do that since there is no similar function to head() for lists in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printList(List):\n",
    "    for i in range(5):\n",
    "        print(List[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweets after cleaning:\n",
      "\n",
      "gas by my house hit  im going to chapel hill on sat \n",
      "with j davlar th main rivals are team poland hopefully we an make it a successful end to a tough week of training tomorrow\n",
      "   i just watched it sridevis comeback u remember her from the s sun mornings on nta \n",
      "one of my best th graders kory was excited after his touchdown today he did the victor cruzlol \n",
      " i didnt want to just pop up but yep we have chapel hill next wednesday you should come and shes great ill tell her you asked\n",
      "\n",
      "Negative tweets after cleaning:\n",
      "\n",
      "theo walcott is still shit watch rafa and johnny deal with him on saturday\n",
      "its not that im a gsp fan i just hate nick diaz cant wait for february\n",
      "iranian general says israels iron dome cant deal with their missiles keep talking like that and we may end up finding out\n",
      "talking about acts  sats deciding where i want to go to college applying to colleges and everything about college stresses me out\n",
      "they may have a superbowl in dallas but dallas aint winning a superbowl not with that quarterback and owner  \n",
      "\n",
      "Neutral tweets after cleaning:\n",
      "\n",
      "tehran mon amour obama tried to establish ties with the mullahs  via  no barack obama  vote mitt romney\n",
      "i sat through this whole movie just for harry and ron at christmas ohlawd\n",
      "why is happy valentines day trending its on the th of february not th of june smh\n",
      "im bringing the monster load of candy tomorrow i just hope it doesnt get all squiched\n",
      "apple software retail chiefs out in overhaul san francisco apple inc ceo tim cook on monday replaced the heads \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive tweets after cleaning:\\n\")\n",
    "printList(positive_tweet)\n",
    "print(\"Negative tweets after cleaning:\\n\")\n",
    "printList(negative_tweet)\n",
    "print(\"Neutral tweets after cleaning:\\n\")\n",
    "printList(neutral_tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use word_tokenize from nltk to tokenize the tweets in order to remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/nikolas/nltk_data'\n    - '/home/nikolas/anaconda3/nltk_data'\n    - '/home/nikolas/anaconda3/share/nltk_data'\n    - '/home/nikolas/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1166dbd7ed7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpositive_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_tweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                            \u001b[0;31m#for every tweet in list of positive tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpositive_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_tweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#tokenize the tweet and append it to the list of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                                                                 \u001b[0;31m#now every tweet is a list of tokens and positive tokens is a list of list of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     return [\n\u001b[1;32m    145\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/nikolas/nltk_data'\n    - '/home/nikolas/anaconda3/nltk_data'\n    - '/home/nikolas/anaconda3/share/nltk_data'\n    - '/home/nikolas/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "positive_tokens = []\n",
    "for x in range(len(positive_tweet)):                            #for every tweet in list of positive tweets\n",
    "    positive_tokens.append(word_tokenize(positive_tweet[x]))    #tokenize the tweet and append it to the list of tokens\n",
    "                                                                #now every tweet is a list of tokens and positive tokens is a list of list of tokens\n",
    "\n",
    "negative_tokens = []\n",
    "for x in range(len(negative_tweet)):\n",
    "    negative_tokens.append(word_tokenize(negative_tweet[x]))\n",
    "\n",
    "neutral_tokens = []\n",
    "for x in range(len(neutral_tweet)):\n",
    "    neutral_tokens.append(word_tokenize(neutral_tweet[x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive tweets after tokenization:\\n\")\n",
    "printList(positive_tokens)\n",
    "print(\"Negative tweets after tokenization:\\n\")\n",
    "printList(negative_tokens)\n",
    "print(\"Neutral tweets after tokenization:\\n\")\n",
    "printList(neutral_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the stopwords from the nltk.corpus in order to remove the stopwords from the tokenized tweets and finish the cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#positive_tokens is a list of list of tokens\n",
    "\n",
    "filtered_positive_words = []\n",
    "filtered_positive_sentences = []\n",
    "for sentence in range(len(positive_tokens)):                                    #for every list-tweet in tokens\n",
    "    for word in range(len(positive_tokens[sentence])):                          #take every token-word of the tweet\n",
    "        if positive_tokens[sentence][word] not in stopwords.words('english'):   #if the token is not a stopword\n",
    "            filtered_positive_words.append(positive_tokens[sentence][word])     #append the word to the list of filtered positive words\n",
    "            \n",
    "    filtered_positive_sentences.append(filtered_positive_words)                 #append the list of non stopwords of the tweet in a list\n",
    "    filtered_positive_words = []                                                #initialize again the list for the next tweet in the positive_tokens list\n",
    "                                                                                #here we create a new positive_tokens list of list of tokens that there are\n",
    "                                                                                #no stopwords  \n",
    "\n",
    "filtered_negative_words = []\n",
    "filtered_negative_sentences = []\n",
    "for sentence in range(len(negative_tokens)):\n",
    "    for word in range(len(negative_tokens[sentence])):\n",
    "        if negative_tokens[sentence][word] not in stopwords.words('english'):\n",
    "            filtered_negative_words.append(negative_tokens[sentence][word])\n",
    "            \n",
    "    filtered_negative_sentences.append(filtered_negative_words)\n",
    "    filtered_negative_words = []            \n",
    "\n",
    "filtered_neutral_words = []\n",
    "filtered_neutral_sentences = []\n",
    "for sentence in range(len(neutral_tokens)):\n",
    "    for word in range(len(neutral_tokens[sentence])):\n",
    "        if neutral_tokens[sentence][word] not in stopwords.words('english'):\n",
    "            filtered_neutral_words.append(neutral_tokens[sentence][word])\n",
    "            \n",
    "    filtered_neutral_sentences.append(filtered_neutral_words)\n",
    "    filtered_neutral_words = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive tweets after removing stopwords:\\n\")\n",
    "printList(filtered_positive_sentences)\n",
    "print(\"Negative tweets after removing stopwords:\\n\")\n",
    "printList(filtered_negative_sentences)\n",
    "print(\"Neutral tweets after removing stopwords:\\n\")\n",
    "printList(filtered_neutral_sentences)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PorterStemmer from nltk.stem in order to do the stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()                                                                              #initialize the stemmer\n",
    "                                                                                                        \n",
    "for sentence in range(len(filtered_positive_sentences)):                                               #for every list-tweet in tokens in filtered_positive_sentences[]\n",
    "    for word in range(len(filtered_positive_sentences[sentence])):                                     #take every token-word of the tweet\n",
    "        filtered_positive_sentences[sentence][word] = stemmer.stem(filtered_positive_sentences[sentence][word])\n",
    "                                                                                                       # stemme the word and put it back in its place\n",
    "\n",
    "for sentence in range(len(filtered_negative_sentences)):\n",
    "    for word in range(len(filtered_negative_sentences[sentence])):\n",
    "        filtered_negative_sentences[sentence][word] = stemmer.stem(filtered_negative_sentences[sentence][word])\n",
    "\n",
    "for sentence in range(len(filtered_neutral_sentences)):\n",
    "    for word in range(len(filtered_neutral_sentences[sentence])):\n",
    "        filtered_neutral_sentences[sentence][word] = stemmer.stem(filtered_neutral_sentences[sentence][word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive tweets after stemming:\\n\")\n",
    "printList(filtered_positive_sentences)\n",
    "print(\"Negative tweets after stemming:\\n\")\n",
    "printList(filtered_negative_sentences)\n",
    "print(\"Neutral tweets after stemming:\\n\")\n",
    "printList(filtered_neutral_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we finished cleaning and stemming we will use Counter from collections in order to count and find the most common words in positive, negative, neutral and all the tweets. After the counting we generate a wordcloud for all 4 categories. After running each box we can see at the bottom 8 of the most common words in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(\"Counting most common words:\")\n",
    "print()\n",
    "\n",
    "all_tweets = []                                                    #we will use this list to store all cleaned tweets and count the most common words\n",
    "\n",
    "tweet_list = []\n",
    "split_it_pos = []\n",
    "for x in range(len(filtered_positive_sentences)):                  #for every list inside the filtered_positive_sentences list\n",
    "\ttweet_list.append(' '.join(filtered_positive_sentences[x]))    #take the tokens separated with commas and make it all a tweet-string again\n",
    "\n",
    "string = ' '.join(tweet_list)                                      #make a string from all the tweets in the list tweet_list\n",
    "split_it = string.split()                                          #split the string in words and save a list of them in split_it \n",
    "print (\"Most common words after cleaning for positive tweets\")\n",
    "counter = Counter(split_it)                                        #we take the dictionary \n",
    "print(counter.most_common(8))                                      #count the most common words from split_it\n",
    "print()\n",
    "\n",
    "all_tweets.append(tweet_list)\n",
    "\n",
    "from wordcloud import WordCloud\t# to install wordcloud you need to do as follows.\n",
    "# git clone https://github.com/amueller/word_cloud.git\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(counter)  #generate wordcloud from frequencies\n",
    "wordcloud.to_file(\"./images/wordclouds/mywc_positive.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the wordcloud that we created from the positive tweets and the frequencies of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./images/wordclouds/mywc_positive.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have also a bar chart in order to see the most common words in the positive tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(\"./images/bar_charts/most_common_words_in_positive_tweets.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same proccess as above for the negative, the neutral and all the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = []\n",
    "split_it_pos = []\n",
    "for x in range(len(filtered_negative_sentences)):                  #for every list inside the filtered_negative_sentences list\n",
    "\ttweet_list.append(' '.join(filtered_negative_sentences[x]))    #take the tokens separated with commas and make it all a tweet-string again\n",
    "\n",
    "string = ' '.join(tweet_list)                                      #make a string from all the tweets in the list tweet_list\n",
    "split_it = string.split()                                          #split the string in words and save a list of them in split_it \n",
    "print (\"Most common words after cleaning for negative tweets\")\n",
    "counter = Counter(split_it)                                        #we take the dictionary \n",
    "print(counter.most_common(8))                                      #count the most common words from split_it\n",
    "print()\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(counter)  #generate wordcloud from frequencies\n",
    "wordcloud.to_file(\"./images/wordclouds/mywc_negative.png\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the wordcloud that we created from the negative tweets and the frequencies of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./images/wordclouds/mywc_negative.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have also a bar chart in order to see the most common words in the negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./images/bar_charts/most_common_words_in_negative_tweets.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the neutral tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets.append(tweet_list)\n",
    "\n",
    "tweet_list = []\n",
    "split_it_pos = []\n",
    "for x in range(len(filtered_neutral_sentences)):                  #for every list inside the filtered_neutral_sentences list\n",
    "\ttweet_list.append(' '.join(filtered_neutral_sentences[x]))    #take the tokens separated with commas and make it all a tweet-string again\n",
    "\n",
    "string = ' '.join(tweet_list)                                      #make a string from all the tweets in the list tweet_list\n",
    "split_it = string.split()                                          #split the string in words and save a list of them in split_it \n",
    "print (\"Most common words after cleaning for neutral tweets\")\n",
    "counter = Counter(split_it)                                        #we take the dictionary \n",
    "print(counter.most_common(8))                                      #count the most common words from split_it\n",
    "print()\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(counter)  #generate wordcloud from frequencies\n",
    "wordcloud.to_file(\"./images/wordclouds/mywc_neutral.png\")                                               # Export my wordcloud image \n",
    "\n",
    "all_tweets.append(tweet_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the wordcloud that we created from the neutral tweets and the frequencies of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./images/wordclouds/mywc_neutral.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have also a bar chart in order to see the most common words in the neutral tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./images/bar_charts/most_common_words_in_neutral_tweets.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use removeNestings() function in order to get the final_list with all the tweets since all_tweets is a list with 3 lists inside it, one for every category of sentiment with all the tweets in this category. In this way we can find the most common words in all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_list = []\n",
    "def removeNestings(l):\n",
    "    for i in l: \n",
    "        if type(i) == list: \n",
    "            removeNestings(i) \n",
    "        else: \n",
    "            final_list.append(i) \n",
    "\n",
    "removeNestings(all_tweets)                                       #we remove the nesting and now we have one big list with all the tweets\n",
    "string = ' '.join(final_list)                              \n",
    "split_it = string.split()\n",
    "\n",
    "print (\"Most common words after cleaning for all tweets\")\n",
    "counter = Counter(split_it)                                        #we take the dictionary \n",
    "print(counter.most_common(8))                                      #count the most common words from split_it\n",
    "print()\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(counter)  #generate wordcloud from frequencies\n",
    "wordcloud.to_file(\"./images/wordclouds/mywc_all.png\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the wordcloud that we created from all tweets and the frequencies of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./images/wordclouds/mywc_all.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have also a bar chart in order to see the most common words in all tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./images/bar_charts/most_common_words_in_all_tweets.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part below we will do the vectorization and the classification. We will use three methods: bag of words, tfidf and word embeddings. We will also need at some point to do some cleaning of the data that's why we will use some functions that we created. At the part above we didn't use these functions because we wanted to do it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(dataframe):\n",
    "    tweets = []                                                        \n",
    "    for x in range(len(dataframe)):                                 \n",
    "        tweets.append(dataframe['Tweet'][x])                     #append only the Tweet column to the list\n",
    "\n",
    "    for x in range(len(tweets)):                                                                        #for every tweet in the list of positive tweets\n",
    "        tweets[x] = re.sub('http://\\S+|https://\\S+', '', tweets[x])                                     #remove the urls\n",
    "        tweets[x] = re.sub('@\\S+', '', tweets[x])                                                       #remove the tags\n",
    "        tweets[x] = re.sub('[0-9`~!@#\\$%^&\\*\\(\\)\\-\\_\\=\\+\\[\\]\\{\\}\\\\\\|\\;\\:<\\,.\\>\\/\\?\\'\\\"]','', tweets[x]) #remove the symbols\n",
    "        tweets[x] = tweets[x].lower()\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def removeStopwords(tweets):\n",
    "    tokens = []\n",
    "    for x in range(len(tweets)):                         #for every tweet in list of tweets\n",
    "        tokens.append(word_tokenize(tweets[x]))          #tokenize the tweet and append it to the list of tokens\n",
    "                                                        #now every tweet is a list of tokens and tokens is a list of list of tokens\n",
    "\n",
    "    filtered_words = []\n",
    "    filtered_sentences = []\n",
    "    for sentence in range(len(tokens)):                                    #for every list-tweet in tokens\n",
    "        for word in range(len(tokens[sentence])):                          #take every token-word of the tweet\n",
    "            if tokens[sentence][word] not in stopwords.words('english'):   #if the token is not a stopword\n",
    "                filtered_words.append(tokens[sentence][word])     #append the word to the list of filtered positive words\n",
    "                \n",
    "        filtered_sentences.append(filtered_words)                 #append the list of non stopwords of the tweet in a list\n",
    "        filtered_words = []                                                #initialize again the list for the next tweet in the positive_tokens list\n",
    "                                                                                    #here we create a new positive_tokens list of list of tokens that there are\n",
    "                                                                                    #no stopwords  \n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function below is called from the bow_train_split where we use the bag of words method in the train data using train_test_split(). We used this way in order to have results quickly and later we will use the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "def bag_of_words(tweets):\n",
    "    bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "    index = 0\n",
    "    for tweet in tweets:\n",
    "        tweets[index] = ' '.join(tweet)\n",
    "        index = index + 1  \n",
    "    bow_xtrain = bow_vectorizer.fit_transform(tweets)  #TWEETS : a list with the actual tweets \n",
    "    return bow_xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words with train split. We will first do preproccessing --> removePuctuation, removeStopwords and create a bag of words model with the bag_of_words() function that we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import pandas as pd\n",
    "#train_file = r'./small_train.tsv'\n",
    "train_file = r'../twitter_data/train2017.tsv'\n",
    "\n",
    "train_dataframe = pd.read_csv(train_file, sep='\\t', names=['ID1','ID2','Sentiment','Tweet'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Preproccessing (train tweets)\n",
    "train_tweets = removePunctuation(train_dataframe)  \n",
    "filtered_train_tweets = removeStopwords(train_tweets)\n",
    "bow_xtrain = bag_of_words(filtered_train_tweets)\n",
    "\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(bow_xtrain, train_dataframe['Sentiment'], random_state=42, test_size=0.2) #input for this method is any array of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preproccessing we will do the classification. First we will use KNN with n_neighbors=1 and then SVM. We use both accuracy_score() and f1_score() to see the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification with KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(xtrain_bow , ytrain)\n",
    "pred_Y = knn.predict(xvalid_bow)\n",
    "\n",
    "# compute classification accuracy for the logistic regression model\n",
    "from sklearn import metrics\n",
    "print(\"Bag of words/KNN\\nmetrics.accuracy_score(yvalid, pred_Y)=\")\n",
    "print(metrics.accuracy_score(yvalid, pred_Y))\n",
    "print(\"\\n\")\n",
    "\"\"\"\n",
    "print(\"yvalid=\")\n",
    "print(yvalid)\n",
    "print(\"pred_Y=\")\n",
    "print(pred_Y)\n",
    "\"\"\"\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "print(\"Bag of words/KNN\\nf1_score(average=micro)=\")\n",
    "print(f1_score(yvalid, pred_Y, average='micro', labels=np.unique(pred_Y))) #seems the same as with accuracy\n",
    "print(\"\\n\")\n",
    "\n",
    "#Classification with SVM\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc = svc.fit(xtrain_bow, ytrain)\n",
    "pred_Y = svc.predict(xvalid_bow)\n",
    "\n",
    "print(\"Bag of words/SVM\\nf1_score(average=macro)=\")\n",
    "print(f1_score(yvalid, pred_Y, average='macro'))\n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use now bag of words with train and test data. Below we do the preproccessing and later we will do the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "#train_file = r'./small_train.tsv'\n",
    "#test_file = r'./small_test.tsv'\n",
    "#test_solutions = r'./small_test_solutions.tsv'\n",
    "\n",
    "train_file = r'../twitter_data/train2017.tsv'\n",
    "test_file = r'../twitter_data/test2017.tsv'\n",
    "test_solutions = r'../twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt'\n",
    "\n",
    "train_dataframe = pd.read_csv(train_file, sep='\\t', names=['ID1','ID2','Sentiment','Tweet'])\n",
    "test_dataframe = pd.read_csv(test_file, sep='\\t', names=['ID1','ID2','Sentiment','Tweet'])\n",
    "test_solutions_dataframe = pd.read_csv(test_solutions, sep='\\t', names=['ID','Sentiment'])\n",
    "\n",
    "#Preproccessing (train tweets)\n",
    "train_tweets = removePunctuation(train_dataframe)  \n",
    "filtered_train_tweets = removeStopwords(train_tweets)\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "index = 0\n",
    "for tweet in filtered_train_tweets:\n",
    "    filtered_train_tweets[index] = ' '.join(tweet)\n",
    "    index = index + 1  \n",
    "xtrain_bow = bow_vectorizer.fit_transform(filtered_train_tweets)  #TWEETS : a list with the actual tweets\n",
    "\n",
    "#Preproccessing (test tweets)\n",
    "test_tweets = removePunctuation(test_dataframe)\n",
    "filtered_test_tweets = removeStopwords(test_tweets)\n",
    "\n",
    "index = 0\n",
    "for tweet in filtered_test_tweets:\n",
    "    filtered_test_tweets[index] = ' '.join(tweet)\n",
    "    index = index + 1  \n",
    "xvalid_bow = bow_vectorizer.transform(filtered_test_tweets) # transform is needed to have the same dimension like xtrain_bow\n",
    "\n",
    "#Calculate ytrain\n",
    "train_sentiment = []\n",
    "for x in range(len(train_dataframe)): \n",
    "    train_sentiment.append(train_dataframe['Sentiment'][x])\n",
    "ytrain = np.array(train_sentiment)\n",
    "\n",
    "#Calculate yvalid\n",
    "test_sentiment = []\n",
    "for x in range(len(test_solutions_dataframe)): \n",
    "    test_sentiment.append(test_solutions_dataframe['Sentiment'][x])\n",
    "yvalid = np.array(test_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification with KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(xtrain_bow , ytrain)\n",
    "pred_Y = knn.predict(xvalid_bow)\n",
    "\n",
    "# compute classification accuracy for the logistic regression model\n",
    "from sklearn import metrics\n",
    "print(\"Bag of words/KNN\\nmetrics.accuracy_score(yvalid, pred_Y)=\")\n",
    "print(metrics.accuracy_score(yvalid, pred_Y))\n",
    "print(\"\\n\")\n",
    "\"\"\"\n",
    "print(\"yvalid=\")\n",
    "print(yvalid)\n",
    "print(\"pred_Y=\")\n",
    "print(pred_Y)\n",
    "\"\"\"\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "print(\"Bag of words/KNN\\nf1_score(average=micro)=\")\n",
    "print(f1_score(yvalid, pred_Y, average='micro', labels=np.unique(pred_Y))) #seems the same as with accuracy\n",
    "print(\"\\n\")\n",
    "\n",
    "#Classification with SVM\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc = svc.fit(xtrain_bow, ytrain)\n",
    "pred_Y = svc.predict(xvalid_bow)\n",
    "\n",
    "print(\"Bag of words/SVM\\nf1_score(average=macro)=\")\n",
    "print(f1_score(yvalid, pred_Y, average='macro'))\n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the different results that bag of words produced with different values in Knn's n_neighbors which is the number of neighbors. We also put the result from svm in order to compare it with the one from Knn. The green is the best value and the red is the words value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./images/classification/bow.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Tfidf() function that we created and it makes, fit_transforms and returns a tfidf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "def Tfidf(tweets):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "    index = 0\n",
    "    for tweet in tweets:\n",
    "        tweets[index] = ' '.join(tweet)\n",
    "        index = index + 1  \n",
    "    tfidf = tfidf_vectorizer.fit_transform(tweets)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidf with train_test_split(). \n",
    "Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import pandas as pd\n",
    "train_file = r'../twitter_data/train2017.tsv'\n",
    "\n",
    "train_dataframe = pd.read_csv(train_file, sep='\\t', names=['ID1','ID2','Sentiment','Tweet'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Preproccessing (train tweets)\n",
    "train_tweets = removePunctuation(train_dataframe)  \n",
    "filtered_train_tweets = removeStopwords(train_tweets)\n",
    "tfidf_xtrain = Tfidf(filtered_train_tweets)\n",
    "\n",
    "xtrain_tfidf, xvalid_tfidf, ytrain, yvalid = train_test_split(tfidf_xtrain, train_dataframe['Sentiment'], random_state=42, test_size=0.2) #input for this method is any array of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification with KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(xtrain_tfidf , ytrain)\n",
    "pred_Y = knn.predict(xvalid_tfidf)\n",
    "\n",
    "# compute classification accuracy for the logistic regression model\n",
    "from sklearn import metrics\n",
    "print(\"TFIDF/KNN\\nmetrics.accuracy_score(yvalid, pred_Y)=\")\n",
    "print(metrics.accuracy_score(yvalid, pred_Y))\n",
    "print(\"\\n\")\n",
    "\"\"\"\n",
    "print(\"yvalid=\")\n",
    "print(yvalid)\n",
    "print(\"pred_Y=\")\n",
    "print(pred_Y)\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "print(\"TFIDF/KNN\\nf1_score(average=micro)=\")\n",
    "print(f1_score(yvalid, pred_Y, average='micro', labels=np.unique(pred_Y))) #seems the same as with accuracy\n",
    "print(\"\\n\")\n",
    "\n",
    "#Classification with SVM\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc = svc.fit(xtrain_tfidf, ytrain)\n",
    "pred_Y = svc.predict(xvalid_tfidf)\n",
    "\n",
    "print(\"TFIDF/SVM\\nf1_score(average=macro)=\")\n",
    "print(f1_score(yvalid, pred_Y, average='macro'))\n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidf with train test data. Preproccessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_file = r'../twitter_data/train2017.tsv'\n",
    "test_file = r'../twitter_data/test2017.tsv'\n",
    "test_solutions = r'../twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt'\n",
    "\n",
    "train_dataframe = pd.read_csv(train_file, sep='\\t', names=['ID1','ID2','Sentiment','Tweet'])\n",
    "test_dataframe = pd.read_csv(test_file, sep='\\t', names=['ID1','ID2','Sentiment','Tweet'])\n",
    "test_solutions_dataframe = pd.read_csv(test_solutions, sep='\\t', names=['ID','Sentiment'])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Preproccessing (train tweets)\n",
    "train_tweets = removePunctuation(train_dataframe)  \n",
    "filtered_train_tweets = removeStopwords(train_tweets)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english') \n",
    "index = 0\n",
    "for tweet in filtered_train_tweets:\n",
    "    filtered_train_tweets[index] = ' '.join(tweet)\n",
    "    index = index + 1  \n",
    "xtrain_tfidf = tfidf_vectorizer.fit_transform(filtered_train_tweets)  #TWEETS : a list with the actual tweets\n",
    "\n",
    "#Preproccessing (test tweets)\n",
    "test_tweets = removePunctuation(test_dataframe)\n",
    "filtered_test_tweets = removeStopwords(test_tweets)\n",
    "\n",
    "index = 0\n",
    "for tweet in filtered_test_tweets:\n",
    "    filtered_test_tweets[index] = ' '.join(tweet)\n",
    "    index = index + 1  \n",
    "xvalid_tfidf = tfidf_vectorizer.transform(filtered_test_tweets) # transform is needed to have the same dimension like xtrain_bow\n",
    "\n",
    "#Calculate ytrain\n",
    "train_sentiment = []\n",
    "for x in range(len(train_dataframe)): \n",
    "    train_sentiment.append(train_dataframe['Sentiment'][x])\n",
    "ytrain = np.array(train_sentiment)\n",
    "\n",
    "#Calculate yvalid\n",
    "test_sentiment = []\n",
    "for x in range(len(test_solutions_dataframe)): \n",
    "    test_sentiment.append(test_solutions_dataframe['Sentiment'][x])\n",
    "yvalid = np.array(test_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Classification with KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(xtrain_tfidf , ytrain)\n",
    "pred_Y = knn.predict(xvalid_tfidf)\n",
    "\n",
    "# compute classification accuracy for the logistic regression model\n",
    "from sklearn import metrics\n",
    "print(\"TFIDF/KNN\\nmetrics.accuracy_score(yvalid, pred_Y)=\")\n",
    "print(metrics.accuracy_score(yvalid, pred_Y))\n",
    "print(\"\\n\")\n",
    "\"\"\"\n",
    "print(\"yvalid=\")\n",
    "print(yvalid)\n",
    "print(\"pred_Y=\")\n",
    "print(pred_Y)\n",
    "\"\"\"\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "print(\"TFIDF/KNN\\nf1_score(average=micro)=\")\n",
    "print(f1_score(yvalid, pred_Y, average='micro', labels=np.unique(pred_Y))) #seems the same as with accuracy\n",
    "print(\"\\n\")\n",
    "\n",
    "#Classification with SVM\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc = svc.fit(xtrain_tfidf, ytrain)\n",
    "pred_Y = svc.predict(xvalid_tfidf)\n",
    "\n",
    "print(\"TFIDF/SVM\\nf1_score(average=macro)=\")\n",
    "print(f1_score(yvalid, pred_Y, average='macro'))\n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, below we can see the different results that tfidf produced with different values in Knn's n_neighbors. We also put the result from svm in order to compare it with the one from Knn. The green is the best value and the red is the words value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./images/classification/tfidf.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have the tweetVectorization() function that is being used in order to transform every tweet into a vector of features. We call this function whenever we want to use word2vec for the vectorization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def tweetVectorization(filtered_sentences):\n",
    "    \"\"\"\n",
    "    model_w2v = Word2Vec(\n",
    "                filtered_sentences,\n",
    "                size=200, # desired no. of features/independent variables\n",
    "                window=5, # context window size\n",
    "                min_count=2,\n",
    "                sg = 1, # 1 for skip-gram model\n",
    "                hs = 0,\n",
    "                negative = 10, # for negative sampling\n",
    "                workers= 2, # no.of cores\n",
    "                seed = 34) \n",
    "\n",
    "    model_w2v.train(filtered_sentences, total_examples= len(filtered_sentences), epochs=20)\n",
    "    \"\"\"\n",
    "    model_w2v = Word2Vec.load('model_w2v.bin')\n",
    "    words = list(model_w2v.wv.vocab)\n",
    "    \n",
    "    dataframe = pd.DataFrame()\n",
    "    tweet_counter = 0\n",
    "    for sentence in range(len(filtered_sentences)):\n",
    "        counter = 0\n",
    "        sum = np.zeros((1,200))\n",
    "        tweet_counter += 1 \n",
    "        for word in range(len(filtered_sentences[sentence])):\n",
    "            counter+=1\n",
    "            #print(filtered_sentences[sentence][word])\n",
    "            if(filtered_sentences[sentence][word] in words):\n",
    "                sum += model_w2v.wv[filtered_sentences[sentence][word]]\n",
    "            else:\n",
    "                # 1) do nothing (such as add a vector of zeroes/zeros)\n",
    "                # 2) add to sum a vector of mean of the current sum (if the first word is \"missing value\" then sum remains zero)\n",
    "                \"\"\"\n",
    "                sum += np.divide(sum, counter)\n",
    "                \"\"\"\n",
    "                # 3) get the max and the min of mean of the current sum and create a vector with random numbers between [min,max]\n",
    "                \n",
    "                min = np.amin(np.divide(sum, counter))\n",
    "                max = np.amax(np.divide(sum, counter))\n",
    "                sum += np.random.uniform(low=min, high=max, size=len(sum))\n",
    "                \n",
    "                # 4) add a vector of random values in a random range\n",
    "                \"\"\"\n",
    "                sum += np.random.uniform(low=-1.5243325, high=1.5068232, size=len(sum))\n",
    "                \"\"\"\n",
    "        if (counter == 0):\n",
    "            counter = 1   \n",
    "        vector = np.divide(sum, counter)\n",
    "        df = pd.DataFrame(vector) \n",
    "        dataframe = dataframe.append(df)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have trained our word2vec model then we have a vocabulary from our given tweets and every word inside it has 200 features. We use tsne in order to have dimensionality reduction and be able to have a plot in 2 dimensions for the vocabulary. Since the size of the vocabulary is too big and the plot is not clear we reduced the words that would appear to the tsne plot in only 140.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./images/tsne_plots/tsne_140.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tsne_plot() function that we used in order to produce the result above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                    xy=(x[i], y[i]),\n",
    "                    xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "                        ha='right',\n",
    "                        va='bottom')\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have the functions that are being used in order to search the vocabularies and extract extra features for our vectors, (this is not the bonus part). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra features from vocabularies (THIS IS NOT THE BONUS)\n",
    "def voc_feature(tweets, lexico_dict):\n",
    "    meanValenceFeature = []\n",
    "\n",
    "    for tweet in tweets:                                    #for every tweet\n",
    "        counter = 0\n",
    "        sum = 0\n",
    "        for word in tweet:                                  #for every word\n",
    "            if (word in lexico_dict['value'].keys()):       #if word is in dictionary\n",
    "                sum += lexico_dict['value'][word]           #count in its' value\n",
    "                counter += 1\n",
    "        if (counter == 0):                                  #if we cannot find any word inside vocabulary we just add zero as extra vocabulary feature for this tweet\n",
    "            counter = 1\n",
    "        new_feature = sum/counter                           #the new_feature is the mean of the sum of words' values\n",
    "        meanValenceFeature.append(new_feature)              #finally, we create this list to hold the extra feature column\n",
    "    return meanValenceFeature\n",
    "\n",
    "#tweetLength\n",
    "def bonus_feature(tweets):\n",
    "    tweetLength = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tweetLength.append(len(tweet))                      #we create this list to hold the extra feature(tweet's length) column\n",
    "    return tweetLength\n",
    "\n",
    "#Max, min valence of tweet\n",
    "def bonus_feature_min_max_valence(tweets, lecixo_dict):\n",
    "\tminValenceFeature = []\n",
    "\tmaxValenceFeature = []\n",
    "\tfor tweet in tweets:                                    #for every tweet\n",
    "\t\tmin_tweet_valence = 5                               #random minimum value\n",
    "\t\tmax_tweet_valence = -5                              #random maximum value\n",
    "\t\tfor word in tweet:                                  #for every word\n",
    "\t\t\tif (word in lecixo_dict['value'].keys()):       #find minimum and maximum valence from all its words\n",
    "\t\t\t\tif (min_tweet_valence > lecixo_dict['value'][word]):\n",
    "\t\t\t\t\tmin_tweet_valence = lecixo_dict['value'][word]\n",
    "\t\t\t\tif (max_tweet_valence < lecixo_dict['value'][word]):\n",
    "\t\t\t\t\tmax_tweet_valence = lecixo_dict['value'][word]\n",
    "\t\tminValenceFeature.append(min_tweet_valence)\n",
    "\t\tmaxValenceFeature.append(max_tweet_valence)\n",
    "\treturn minValenceFeature, maxValenceFeature             #return two lists as the extra features columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the word2vec vectorization with the vocabularies: valence_tweet.txt, affin.txt, generic,txt, val.txt plus we have the bonus. The bonus is that we have put some extra features in the vectors except from the vocabularies. First of all we added the length of every tweet in the vector. We also found the min and max valence for every tweet and put it too. \n",
    "\n",
    "Note: min valence is the smallest valence that we found in the tweet and max valence is the largest valence that we found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec with all vocabularies plus bonus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "\n",
    "train_file = r'../twitter_data/train2017.tsv'\n",
    "test_file = r'../twitter_data/test2017.tsv'\n",
    "test_solutions = r'../twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt'\n",
    "\n",
    "train_dataframe = pd.read_csv(train_file, sep='\\t', names=['ID1','ID2','Sentiment','Tweet'])\n",
    "test_dataframe = pd.read_csv(test_file, sep='\\t', names=['ID1','ID2','Sentiment','Tweet'])\n",
    "test_solutions_dataframe = pd.read_csv(test_solutions, sep='\\t', names=['ID','Sentiment'])\n",
    "\n",
    "train_tweets = []                                                        \n",
    "for x in range(len(train_dataframe)):                                 \n",
    "    train_tweets.append(train_dataframe['Tweet'][x])                    #append only the Tweet column to the list\n",
    "\n",
    "train_tokens = []\n",
    "for x in range(len(train_tweets)):                                      #for every tweet in list of tweets\n",
    "    train_tokens.append(word_tokenize(train_tweets[x]))                 #tokenize the tweet and append it to the list of tokens\n",
    "\n",
    "test_tweets = []                                                        \n",
    "for x in range(len(test_dataframe)):                                 \n",
    "    test_tweets.append(test_dataframe['Tweet'][x])                      #append only the Tweet column to the list\n",
    "\n",
    "test_tokens = []\n",
    "for x in range(len(test_tweets)):                                       #for every tweet in list of tweets\n",
    "    test_tokens.append(word_tokenize(test_tweets[x]))                   #tokenize the tweet and append it to the list of tokens\n",
    "\n",
    "train_voc_features = []                                                 #this will be a list of lists and every list inside it will be a new column for the vector         \n",
    "test_voc_features = []                                                  #this will be a list of lists and every list inside it will be a new column for the vector\n",
    "\n",
    "lexico = r'../lexica/emotweet/valence_tweet.txt'                           \n",
    "lexico_dataframe = pd.read_csv(lexico, sep='\\t', names=['word', 'value']) #read lexico as a dataframe\n",
    "lexico_dict = lexico_dataframe.set_index('word').to_dict(orient='dict')   #convert it to a dictionary in order to be much faster when we search a word inside it\n",
    "#print(lexico_dict['value']['nba'])\n",
    "train_voc_features.append(voc_feature(train_tokens,lexico_dict))          #call voc_feature() and append the list with the features from this lexico to the list of all features\n",
    "test_voc_features.append(voc_feature(test_tokens,lexico_dict))            #do the same as above for the test data\n",
    "#Bonus features (Min, Max Tweet Valence)\n",
    "train_minValenceFeature, train_maxValenceFeature = bonus_feature_min_max_valence(train_tokens, lexico_dict)  #find min and max valence from all tokens from a tweet in lexico\n",
    "test_minValenceFeature, test_maxValenceFeature = bonus_feature_min_max_valence(test_tokens, lexico_dict)     #we do the same for test data\n",
    "train_voc_features.append(train_minValenceFeature)                        #just append the new column with features\n",
    "train_voc_features.append(train_maxValenceFeature)                        #just append the new column with features\n",
    "test_voc_features.append(test_minValenceFeature)                          #just append the new column with features\n",
    "test_voc_features.append(test_maxValenceFeature)                          #just append the new column with features\n",
    "  \n",
    "lexico = r'../lexica/affin/affin.txt'\n",
    "lexico_dataframe = pd.read_csv(lexico, sep='\\t', names=['word', 'value'])\n",
    "lexico_dict = lexico_dataframe.set_index('word').to_dict(orient='dict')\n",
    "#print(lexico_dict['value']['nba'])\n",
    "train_voc_features.append(voc_feature(train_tokens,lexico_dict))\n",
    "test_voc_features.append(voc_feature(test_tokens,lexico_dict))\n",
    "#Bonus features (Min, Max Tweet Valence)\n",
    "train_minValenceFeature, train_maxValenceFeature = bonus_feature_min_max_valence(train_tokens, lexico_dict)\n",
    "test_minValenceFeature, test_maxValenceFeature = bonus_feature_min_max_valence(test_tokens, lexico_dict)\n",
    "train_voc_features.append(train_minValenceFeature)\n",
    "train_voc_features.append(train_maxValenceFeature)\n",
    "test_voc_features.append(test_minValenceFeature)\n",
    "test_voc_features.append(test_maxValenceFeature)\n",
    "\n",
    "lexico = r'../lexica/generic/generic.txt'\n",
    "lexico_dataframe = pd.read_csv(lexico, sep='\\t', names=['word', 'value'])\n",
    "lexico_dict = lexico_dataframe.set_index('word').to_dict(orient='dict')\n",
    "#print(lexico_dict['value']['nba'])\n",
    "train_voc_features.append(voc_feature(train_tokens,lexico_dict))\n",
    "test_voc_features.append(voc_feature(test_tokens,lexico_dict))\n",
    "#Bonus features (Min, Max Tweet Valence)\n",
    "train_minValenceFeature, train_maxValenceFeature = bonus_feature_min_max_valence(train_tokens, lexico_dict)\n",
    "test_minValenceFeature, test_maxValenceFeature = bonus_feature_min_max_valence(test_tokens, lexico_dict)\n",
    "train_voc_features.append(train_minValenceFeature)\n",
    "train_voc_features.append(train_maxValenceFeature)\n",
    "test_voc_features.append(test_minValenceFeature)\n",
    "test_voc_features.append(test_maxValenceFeature)\n",
    "\n",
    "lexico = r'../lexica/nrctag/val.txt'\n",
    "lexico_dataframe = pd.read_csv(lexico, sep='\\t', names=['word', 'value'])\n",
    "lexico_dict = lexico_dataframe.set_index('word').to_dict(orient='dict')\n",
    "#print(lexico_dict['value']['nba'])\n",
    "train_voc_features.append(voc_feature(train_tokens, lexico_dict))\n",
    "test_voc_features.append(voc_feature(test_tokens, lexico_dict))\n",
    "#Bonus features (Min, Max Tweet Valence)\n",
    "train_minValenceFeature, train_maxValenceFeature = bonus_feature_min_max_valence(train_tokens, lexico_dict)\n",
    "test_minValenceFeature, test_maxValenceFeature = bonus_feature_min_max_valence(test_tokens, lexico_dict)\n",
    "train_voc_features.append(train_minValenceFeature)\n",
    "train_voc_features.append(train_maxValenceFeature)\n",
    "test_voc_features.append(test_minValenceFeature)\n",
    "test_voc_features.append(test_maxValenceFeature)\n",
    "\n",
    "#Bonus Features\n",
    "train_voc_features.append(bonus_feature(train_tokens))  #this is another extra feature that we will use, it is the length of the tweet\n",
    "test_voc_features.append(bonus_feature(test_tokens))     \n",
    "\n",
    "#Preproccessing (train tweets)\n",
    "train_tweets = removePunctuation(train_dataframe)       #remove punctuation from train tweets\n",
    "filtered_train_tweets = removeStopwords(train_tweets)   #remove english stopwords from train tweets\n",
    "\n",
    "#WORD2VEC\n",
    "train_vector_dataframe = tweetVectorization(filtered_train_tweets)  #create a vector for every train tweet\n",
    "\n",
    "#Preproccessing (test tweets) \n",
    "test_tweets = removePunctuation(test_dataframe)         #remove punctuation from test tweets\n",
    "filtered_test_tweets = removeStopwords(test_tweets)     #remove english stopwords from test tweets\n",
    "\n",
    "#WORD2VEC\n",
    "test_vector_dataframe = tweetVectorization(filtered_test_tweets)    #create a vector for every test tweet\n",
    "\n",
    "train_X = train_vector_dataframe.values                 #we take the values of the vector as a numpy array\n",
    "\n",
    "for x in range(len(train_voc_features)):                #for every list of features inside the list train_voc_features\n",
    "    new_features = np.array(train_voc_features[x])      #we convert the list to a numpy array\n",
    "    new_features = np.reshape(new_features,(len(train_voc_features[x]),1)) #we reshape from (200,) to(200,1)\n",
    "    train_X = np.append(train_X, new_features,axis=1)   #now we append this list as a column in the train_X numpy array\n",
    "\n",
    "#Extra features\n",
    "test_X = test_vector_dataframe.values                   #we take the values of the vector as a numpy array\n",
    "\n",
    "for x in range(len(test_voc_features)):\n",
    "    new_features = np.array(test_voc_features[x])\n",
    "    new_features = np.reshape(new_features,(len(test_voc_features[x]),1))\n",
    "    test_X = np.append(test_X, new_features,axis=1)\n",
    "\n",
    "train_sentiment = []\n",
    "for x in range(len(train_dataframe)):                       #for every tweet in the train_dataframe\n",
    "    train_sentiment.append(train_dataframe['Sentiment'][x]) #we take the sentiment and append it in a list\n",
    "train_Y = np.array(train_sentiment)                         #we convert this list in a numpy array named train_Y as the solutions for the train data\n",
    "\n",
    "test_sentiment = []\n",
    "for x in range(len(test_solutions_dataframe)): \n",
    "    test_sentiment.append(test_solutions_dataframe['Sentiment'][x])\n",
    "test_Y = np.array(test_sentiment)\n",
    "\n",
    "#Classification with KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=30)\n",
    "#WORD2VEC\n",
    "knn.fit(train_X , train_Y)\n",
    "pred_Y = knn.predict(test_X)\n",
    "\n",
    "# compute classification accuracy for the logistic regression model\n",
    "from sklearn import metrics\n",
    "print(\"Word2vec(word embeddings)/KNN\\nmetrics.accuracy_score(test_Y, pred_Y)=\")\n",
    "print(metrics.accuracy_score(test_Y, pred_Y))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"Word2vec(word embeddings)/KNN\\nf1_score(test_Y, pred_Y, average='micro', labels=np.unique(pred_Y))=\")\n",
    "print(f1_score(test_Y, pred_Y, average='micro', labels=np.unique(pred_Y))) #seems the same as with accuracy\n",
    "\n",
    "#Classification with SVM\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc = svc.fit(train_X, train_Y)\n",
    "pred_Y = svc.predict(test_X)\n",
    "\n",
    "print(\"Word2vec(word embeddings)/SVM\\nmetrics.accuracy_score(test_Y, pred_Y)=\")\n",
    "print(metrics.accuracy_score(test_Y, pred_Y))\n",
    "print(\"Word2vec(word embeddings)/SVM\\nf1_score(test_Y, pred_Y, average='macro')=\")\n",
    "print(f1_score(test_Y, pred_Y, average='macro'))\n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have 2 tables in order to see the results that we had while running word2vec: 1) without vocabularies, 2) only with valence_tweet.txt, 3) only with affin.txt, 4) only with generic.txt, 5) only with val.txt, 6) with all vocabularies and 7) with all vocabularies + bonus\n",
    "\n",
    "Note: In Knn the number of neighbors is 1 (n_neighbors=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./images/classification/word2vecknn1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./images/classification/word2vecknn1cont.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above tables that the vocabulary affin.txt produced great results in comparison with the other vocabularies alone. It was also a little strange for us to see that it produced better results for the knn even from the last column with all the vocabularies plus the bonus. Nevertheless the best results in svm classifier are with all vocabularies plus bonus and the knn also produced the almost the best results. We can conclude that if we have all the vocabularies and the extra features from the bonus we have the best results.\n",
    "\n",
    "We compared the results from the knn and svm classifiers and saw a big difference. So we decided that we should search how the knn could produce better results. The number of neighbors that was given to knn was 1 so after trying different values we created the table below. The red was the worst value and the green was the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./images/classification/word2vecwithoutmanyk.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were very good for us since we saw that the knn probably would produce better results with all the vocabularies and the extra features if we would run it with n_neighbors=30 or n_neighbors=100. So we selected the best 4 values (not 100) from the table above and we created a new table. \n",
    "\n",
    "The table below has the final results that knn and svm produced with the different values of n_neighbors in knn. We have to say that the reason we put svm in this table was to see what the average value would be since it does not always produce the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./images/classification/word2vecallmanyk.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we have the final results. As you can see the best results were produced by the Word2Vec. We also observed that bag of words produced better results than tfidf with Knn classifier but tfidf was better (slightly higher) with svm classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./images/classification/finalresults.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
